{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ou0ZKSDWw4Hw"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XBRl10eTwgUD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data_file_name, data_dir='.data/'):\n",
        "    super().__init__()\n",
        "\n",
        "    data_path = os.path.join(data_file_name)\n",
        "\n",
        "    self.data_list = []\n",
        "    self.end_of_text_token = \" <|endoftext|> \"\n",
        "\t\t\n",
        "    with open(data_path) as data_file:\n",
        "      lines = data_file.readlines()\n",
        "      for line in lines:\n",
        "        l = line.replace('\\n', self.end_of_text_token)\n",
        "        self.data_list.append(l)\n",
        "\t\t\n",
        "  def __len__(self):\n",
        "    return len(self.data_list)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return self.data_list[item]\n",
        "\n",
        "def get_data_loader(data_file_name):\n",
        "\tdataset = MyDataset(data_file_name)\n",
        "\tdata_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\treturn data_loader\n",
        "\n",
        "def train(epochs, data_loader, batch_size, tokenizer, model, device):\t\n",
        "\tbatch_counter = 0\n",
        "\tsum_loss = 0.0\n",
        "\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tprint (f'Running {epoch+1} epoch')\n",
        "\n",
        "\t\tfor idx, txt in enumerate(data_loader):\n",
        "\t\t\ttxt = torch.tensor(tokenizer.encode(txt[0]))\n",
        "\t\t\ttxt = txt.unsqueeze(0).to(device)\n",
        "\t\t\toutputs = model(txt, labels=txt)\n",
        "\t\t\tloss, _ = outputs[:2]\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tsum_loss += loss.data\n",
        "\n",
        "\t\t\tif idx%batch_size==0:\n",
        "\t\t\t\tbatch_counter += 1\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tscheduler.step()\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tmodel.zero_grad()\n",
        "\n",
        "\t\t\tif batch_counter == 10:\n",
        "\t\t\t\tprint(f\"Total Loss is {sum_loss}\") #printed after every 10*batch_size\n",
        "\t\t\t\tbatch_counter = 0\n",
        "\t\t\t\tsum_loss = 0.0\n",
        "\n",
        "\treturn model\n",
        "\n",
        "def save_model(model, name):\n",
        "\t\"\"\"\n",
        "\tSummary:\n",
        "\t\tSaving model to the Disk\n",
        "\tParameters:\n",
        "\t\tmodel: Trained model object\n",
        "\t\tname: Name of the model to be saved\n",
        "\t\"\"\"\n",
        "\tprint (\"Saving model to Disk\")\n",
        "\ttorch.save(model.state_dict(), f\"{name}.pt\")\n",
        "\treturn\n",
        "\n",
        "def load_models():\n",
        "\t\"\"\"\n",
        "\tSummary:\n",
        "\t\tLoading Pre-trained model\n",
        "\t\"\"\"\n",
        "\tprint ('Loading/Downloading GPT-2 Model')\n",
        "\ttokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "\tmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "\treturn tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S402J1khwyfN"
      },
      "outputs": [],
      "source": [
        "gpt_tok, gpt_pre = load_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UIn9sJKo2vuD"
      },
      "outputs": [],
      "source": [
        "intent_data = get_data_loader('gpt_data.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cyEitzruD2-1"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "gpt_pre.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jerEM9dYFUaO"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(gpt_pre.parameters(), lr=3e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MkbJeqB6FX9W"
      },
      "outputs": [],
      "source": [
        "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=300, num_training_steps=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnG2-4VFC3md",
        "outputId": "9b6448ff-36a9-489c-e594-047005e2c05d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running 1 epoch\n",
            "Total Loss is 468.53094482421875\n",
            "Total Loss is 312.11077880859375\n",
            "Total Loss is 248.7689971923828\n",
            "Total Loss is 222.0544891357422\n",
            "Total Loss is 232.70681762695312\n",
            "Total Loss is 208.8592529296875\n",
            "Total Loss is 206.02508544921875\n",
            "Total Loss is 229.0137176513672\n",
            "Total Loss is 221.3388671875\n",
            "Total Loss is 228.28688049316406\n",
            "Total Loss is 215.26014709472656\n",
            "Total Loss is 199.99774169921875\n",
            "Total Loss is 229.53857421875\n",
            "Total Loss is 222.13967895507812\n",
            "Total Loss is 213.6986541748047\n",
            "Total Loss is 216.7646484375\n",
            "Total Loss is 227.72793579101562\n",
            "Total Loss is 298.00518798828125\n",
            "Total Loss is 265.0007629394531\n",
            "Total Loss is 256.3049011230469\n",
            "Total Loss is 237.97482299804688\n",
            "Total Loss is 222.09034729003906\n",
            "Total Loss is 226.44760131835938\n",
            "Total Loss is 228.945556640625\n",
            "Total Loss is 213.88426208496094\n",
            "Total Loss is 224.8849334716797\n",
            "Total Loss is 226.03961181640625\n",
            "Total Loss is 222.86502075195312\n",
            "Total Loss is 221.03744506835938\n",
            "Total Loss is 232.37461853027344\n",
            "Total Loss is 216.86953735351562\n",
            "Total Loss is 223.31358337402344\n",
            "Total Loss is 200.0894317626953\n",
            "Total Loss is 212.3856658935547\n",
            "Total Loss is 208.25244140625\n",
            "Total Loss is 213.59678649902344\n",
            "Total Loss is 215.76605224609375\n",
            "Total Loss is 229.74588012695312\n",
            "Total Loss is 222.82005310058594\n",
            "Total Loss is 226.80502319335938\n",
            "Total Loss is 218.28707885742188\n",
            "Total Loss is 216.38331604003906\n",
            "Total Loss is 218.83558654785156\n",
            "Total Loss is 212.64968872070312\n",
            "Total Loss is 232.73780822753906\n",
            "Total Loss is 209.1477813720703\n",
            "Total Loss is 219.51202392578125\n",
            "Total Loss is 205.9535675048828\n",
            "Total Loss is 220.52597045898438\n",
            "Total Loss is 212.88613891601562\n",
            "Total Loss is 217.42636108398438\n",
            "Total Loss is 218.43011474609375\n",
            "Total Loss is 220.8143768310547\n",
            "Total Loss is 228.97845458984375\n",
            "Total Loss is 224.6957244873047\n",
            "Total Loss is 225.4397430419922\n",
            "Total Loss is 214.03526306152344\n",
            "Total Loss is 210.62208557128906\n",
            "Total Loss is 220.71226501464844\n",
            "Total Loss is 231.6928253173828\n",
            "Total Loss is 228.94602966308594\n",
            "Total Loss is 215.81210327148438\n",
            "Total Loss is 215.5374298095703\n",
            "Total Loss is 211.6807861328125\n",
            "Total Loss is 215.4320068359375\n",
            "Total Loss is 219.1068572998047\n",
            "Total Loss is 212.36495971679688\n",
            "Total Loss is 222.530029296875\n",
            "Total Loss is 239.8497772216797\n",
            "Total Loss is 227.30780029296875\n",
            "Total Loss is 225.90904235839844\n",
            "Total Loss is 221.60504150390625\n",
            "Total Loss is 211.60076904296875\n",
            "Total Loss is 198.93240356445312\n",
            "Total Loss is 215.5002899169922\n",
            "Total Loss is 234.59483337402344\n",
            "Total Loss is 218.19558715820312\n",
            "Total Loss is 223.66844177246094\n",
            "Total Loss is 226.38880920410156\n",
            "Total Loss is 212.99220275878906\n",
            "Total Loss is 212.08218383789062\n",
            "Total Loss is 206.282470703125\n",
            "Total Loss is 219.528564453125\n",
            "Total Loss is 213.32687377929688\n",
            "Total Loss is 210.2525634765625\n",
            "Total Loss is 221.35137939453125\n",
            "Total Loss is 217.06349182128906\n",
            "Total Loss is 216.31488037109375\n",
            "Total Loss is 218.5752410888672\n",
            "Total Loss is 209.52105712890625\n",
            "Total Loss is 217.33041381835938\n",
            "Total Loss is 221.9479522705078\n",
            "Total Loss is 195.83763122558594\n",
            "Total Loss is 202.6612548828125\n",
            "Total Loss is 220.28404235839844\n",
            "Total Loss is 216.56149291992188\n",
            "Total Loss is 220.3048095703125\n",
            "Total Loss is 232.92869567871094\n",
            "Total Loss is 237.3417510986328\n",
            "Total Loss is 211.96514892578125\n",
            "Total Loss is 227.36160278320312\n",
            "Total Loss is 213.72866821289062\n",
            "Total Loss is 218.36265563964844\n",
            "Total Loss is 218.04974365234375\n",
            "Total Loss is 209.69332885742188\n",
            "Total Loss is 220.30545043945312\n",
            "Total Loss is 217.09193420410156\n",
            "Total Loss is 214.11105346679688\n",
            "Total Loss is 210.48138427734375\n",
            "Total Loss is 203.42845153808594\n",
            "Total Loss is 216.4001922607422\n",
            "Total Loss is 203.40199279785156\n",
            "Total Loss is 227.0519256591797\n",
            "Total Loss is 218.4094696044922\n",
            "Total Loss is 210.50082397460938\n",
            "Total Loss is 209.5216827392578\n",
            "Total Loss is 211.49710083007812\n",
            "Total Loss is 222.4697265625\n",
            "Total Loss is 221.26051330566406\n",
            "Total Loss is 222.588134765625\n",
            "Total Loss is 212.76605224609375\n",
            "Total Loss is 230.56723022460938\n",
            "Total Loss is 214.95103454589844\n",
            "Total Loss is 216.21841430664062\n",
            "Total Loss is 218.57997131347656\n",
            "Total Loss is 211.8406524658203\n",
            "Total Loss is 204.1272430419922\n",
            "Total Loss is 197.20565795898438\n",
            "Total Loss is 215.1436309814453\n",
            "Total Loss is 230.0523681640625\n",
            "Total Loss is 218.84645080566406\n",
            "Total Loss is 211.35073852539062\n",
            "Total Loss is 227.91278076171875\n",
            "Total Loss is 220.54782104492188\n",
            "Total Loss is 215.96560668945312\n",
            "Total Loss is 226.10023498535156\n",
            "Total Loss is 229.98521423339844\n",
            "Total Loss is 204.07208251953125\n",
            "Total Loss is 241.84432983398438\n",
            "Total Loss is 222.37530517578125\n",
            "Total Loss is 228.42721557617188\n",
            "Total Loss is 222.77987670898438\n",
            "Total Loss is 214.25772094726562\n",
            "Total Loss is 217.62088012695312\n",
            "Total Loss is 208.12506103515625\n",
            "Total Loss is 218.73526000976562\n",
            "Total Loss is 223.61166381835938\n",
            "Total Loss is 222.78428649902344\n",
            "Total Loss is 219.5089111328125\n",
            "Total Loss is 230.17156982421875\n",
            "Total Loss is 219.16078186035156\n",
            "Total Loss is 200.9588623046875\n",
            "Total Loss is 225.21987915039062\n",
            "Total Loss is 229.527099609375\n",
            "Total Loss is 219.44627380371094\n",
            "Total Loss is 220.67640686035156\n",
            "Total Loss is 227.2372589111328\n",
            "Total Loss is 212.8255157470703\n",
            "Total Loss is 218.7652130126953\n",
            "Total Loss is 200.60440063476562\n",
            "Total Loss is 205.2946014404297\n",
            "Total Loss is 222.6287841796875\n",
            "Total Loss is 214.06527709960938\n",
            "Running 2 epoch\n",
            "Total Loss is 201.2618865966797\n",
            "Total Loss is 224.91558837890625\n",
            "Total Loss is 202.1119384765625\n",
            "Total Loss is 213.95709228515625\n",
            "Total Loss is 214.2240753173828\n",
            "Total Loss is 212.68695068359375\n",
            "Total Loss is 217.11697387695312\n",
            "Total Loss is 208.97772216796875\n",
            "Total Loss is 211.2577362060547\n",
            "Total Loss is 210.48129272460938\n",
            "Total Loss is 224.1822052001953\n",
            "Total Loss is 200.51646423339844\n",
            "Total Loss is 215.54995727539062\n",
            "Total Loss is 216.8853302001953\n",
            "Total Loss is 207.04473876953125\n",
            "Total Loss is 205.95994567871094\n",
            "Total Loss is 214.86358642578125\n",
            "Total Loss is 201.52423095703125\n",
            "Total Loss is 205.35992431640625\n",
            "Total Loss is 205.84922790527344\n",
            "Total Loss is 210.06362915039062\n",
            "Total Loss is 213.5232696533203\n",
            "Total Loss is 201.28443908691406\n",
            "Total Loss is 206.2851104736328\n",
            "Total Loss is 206.15570068359375\n",
            "Total Loss is 204.21783447265625\n",
            "Total Loss is 212.3748779296875\n",
            "Total Loss is 208.58811950683594\n",
            "Total Loss is 206.04263305664062\n",
            "Total Loss is 199.19671630859375\n",
            "Total Loss is 201.27638244628906\n",
            "Total Loss is 210.59661865234375\n",
            "Total Loss is 202.00506591796875\n",
            "Total Loss is 199.37124633789062\n",
            "Total Loss is 196.74490356445312\n",
            "Total Loss is 197.49224853515625\n",
            "Total Loss is 216.85569763183594\n",
            "Total Loss is 205.8907470703125\n",
            "Total Loss is 209.9304656982422\n",
            "Total Loss is 221.4076385498047\n",
            "Total Loss is 210.03492736816406\n",
            "Total Loss is 199.81753540039062\n",
            "Total Loss is 205.1865234375\n",
            "Total Loss is 211.78536987304688\n",
            "Total Loss is 202.2621612548828\n",
            "Total Loss is 222.7921905517578\n",
            "Total Loss is 211.5440673828125\n",
            "Total Loss is 213.52984619140625\n",
            "Total Loss is 211.31521606445312\n",
            "Total Loss is 208.69744873046875\n",
            "Total Loss is 220.71507263183594\n",
            "Total Loss is 213.05899047851562\n",
            "Total Loss is 209.6427459716797\n",
            "Total Loss is 203.27243041992188\n",
            "Total Loss is 209.52462768554688\n",
            "Total Loss is 210.2867889404297\n",
            "Total Loss is 195.50140380859375\n",
            "Total Loss is 208.1532440185547\n",
            "Total Loss is 216.8976287841797\n",
            "Total Loss is 218.75025939941406\n",
            "Total Loss is 223.2493438720703\n",
            "Total Loss is 205.26792907714844\n",
            "Total Loss is 200.42596435546875\n",
            "Total Loss is 216.7148895263672\n",
            "Total Loss is 215.52880859375\n",
            "Total Loss is 207.16368103027344\n",
            "Total Loss is 212.17160034179688\n",
            "Total Loss is 204.20938110351562\n",
            "Total Loss is 198.15342712402344\n",
            "Total Loss is 200.45518493652344\n",
            "Total Loss is 192.4873809814453\n",
            "Total Loss is 220.47012329101562\n",
            "Total Loss is 206.4393310546875\n",
            "Total Loss is 192.59097290039062\n",
            "Total Loss is 203.80010986328125\n",
            "Total Loss is 214.97032165527344\n",
            "Total Loss is 210.87767028808594\n",
            "Total Loss is 206.25048828125\n",
            "Total Loss is 209.55731201171875\n",
            "Total Loss is 215.56361389160156\n",
            "Total Loss is 216.78236389160156\n",
            "Total Loss is 224.02664184570312\n",
            "Total Loss is 216.41195678710938\n",
            "Total Loss is 209.02587890625\n",
            "Total Loss is 211.34999084472656\n",
            "Total Loss is 213.3916778564453\n",
            "Total Loss is 199.2962188720703\n",
            "Total Loss is 210.23870849609375\n",
            "Total Loss is 202.38856506347656\n",
            "Total Loss is 202.8693084716797\n",
            "Total Loss is 217.116943359375\n",
            "Total Loss is 192.51565551757812\n",
            "Total Loss is 197.23013305664062\n",
            "Total Loss is 222.21559143066406\n",
            "Total Loss is 205.48956298828125\n",
            "Total Loss is 217.0325469970703\n",
            "Total Loss is 214.37840270996094\n",
            "Total Loss is 215.65232849121094\n",
            "Total Loss is 220.5826416015625\n",
            "Total Loss is 207.22190856933594\n",
            "Total Loss is 205.4608154296875\n",
            "Total Loss is 219.0303955078125\n",
            "Total Loss is 207.98092651367188\n",
            "Total Loss is 208.71876525878906\n",
            "Total Loss is 203.27102661132812\n",
            "Total Loss is 222.2962188720703\n",
            "Total Loss is 205.6238555908203\n",
            "Total Loss is 207.38027954101562\n",
            "Total Loss is 215.71522521972656\n",
            "Total Loss is 203.18020629882812\n",
            "Total Loss is 219.22377014160156\n",
            "Total Loss is 211.53285217285156\n",
            "Total Loss is 204.31326293945312\n",
            "Total Loss is 208.11831665039062\n",
            "Total Loss is 201.9835968017578\n",
            "Total Loss is 211.32266235351562\n",
            "Total Loss is 217.40928649902344\n",
            "Total Loss is 211.0218048095703\n",
            "Total Loss is 194.398681640625\n",
            "Total Loss is 203.53118896484375\n",
            "Total Loss is 216.55496215820312\n",
            "Total Loss is 191.5877685546875\n",
            "Total Loss is 198.17095947265625\n",
            "Total Loss is 208.15399169921875\n",
            "Total Loss is 205.3335418701172\n",
            "Total Loss is 215.67918395996094\n",
            "Total Loss is 190.33978271484375\n",
            "Total Loss is 209.87745666503906\n",
            "Total Loss is 204.36727905273438\n",
            "Total Loss is 211.17828369140625\n",
            "Total Loss is 207.48228454589844\n",
            "Total Loss is 210.80712890625\n",
            "Total Loss is 214.1820831298828\n",
            "Total Loss is 197.19219970703125\n",
            "Total Loss is 220.676025390625\n",
            "Total Loss is 202.41647338867188\n",
            "Total Loss is 219.59915161132812\n",
            "Total Loss is 209.3204345703125\n",
            "Total Loss is 195.5067901611328\n",
            "Total Loss is 205.62950134277344\n",
            "Total Loss is 218.38597106933594\n",
            "Total Loss is 212.76727294921875\n",
            "Total Loss is 210.81405639648438\n",
            "Total Loss is 210.54940795898438\n",
            "Total Loss is 202.9506378173828\n",
            "Total Loss is 200.27468872070312\n",
            "Total Loss is 203.73841857910156\n",
            "Total Loss is 209.5270233154297\n",
            "Total Loss is 221.2403564453125\n",
            "Total Loss is 192.8461456298828\n",
            "Total Loss is 201.82754516601562\n",
            "Total Loss is 199.1429901123047\n",
            "Total Loss is 209.5940399169922\n",
            "Total Loss is 211.52464294433594\n",
            "Total Loss is 208.31919860839844\n",
            "Total Loss is 212.3118133544922\n",
            "Total Loss is 226.16184997558594\n",
            "Total Loss is 205.67674255371094\n",
            "Total Loss is 209.24786376953125\n",
            "Total Loss is 216.3037109375\n",
            "Total Loss is 202.4729766845703\n",
            "Total Loss is 213.31341552734375\n",
            "Total Loss is 217.54991149902344\n",
            "Total Loss is 211.72340393066406\n",
            "Running 3 epoch\n",
            "Total Loss is 203.1848602294922\n",
            "Total Loss is 217.79002380371094\n",
            "Total Loss is 212.60635375976562\n",
            "Total Loss is 201.3565216064453\n",
            "Total Loss is 199.18063354492188\n",
            "Total Loss is 206.40371704101562\n",
            "Total Loss is 217.93350219726562\n",
            "Total Loss is 212.88124084472656\n",
            "Total Loss is 206.492919921875\n",
            "Total Loss is 209.97259521484375\n",
            "Total Loss is 208.087646484375\n",
            "Total Loss is 199.80667114257812\n",
            "Total Loss is 214.13018798828125\n",
            "Total Loss is 215.86294555664062\n",
            "Total Loss is 201.63331604003906\n",
            "Total Loss is 219.50894165039062\n",
            "Total Loss is 204.47007751464844\n",
            "Total Loss is 213.1640625\n",
            "Total Loss is 213.41256713867188\n",
            "Total Loss is 209.12318420410156\n",
            "Total Loss is 213.757080078125\n",
            "Total Loss is 217.5514678955078\n",
            "Total Loss is 195.76617431640625\n",
            "Total Loss is 211.66943359375\n",
            "Total Loss is 209.74806213378906\n",
            "Total Loss is 206.93829345703125\n",
            "Total Loss is 210.94403076171875\n",
            "Total Loss is 201.16098022460938\n",
            "Total Loss is 205.75357055664062\n",
            "Total Loss is 215.76280212402344\n",
            "Total Loss is 218.70338439941406\n",
            "Total Loss is 214.06820678710938\n",
            "Total Loss is 199.00424194335938\n",
            "Total Loss is 188.51577758789062\n",
            "Total Loss is 204.31065368652344\n",
            "Total Loss is 197.12112426757812\n",
            "Total Loss is 216.219970703125\n",
            "Total Loss is 206.18264770507812\n",
            "Total Loss is 211.9250030517578\n",
            "Total Loss is 204.27960205078125\n",
            "Total Loss is 200.0308380126953\n",
            "Total Loss is 212.67572021484375\n",
            "Total Loss is 209.08995056152344\n",
            "Total Loss is 195.1193084716797\n",
            "Total Loss is 207.04527282714844\n",
            "Total Loss is 218.55149841308594\n",
            "Total Loss is 214.9689483642578\n",
            "Total Loss is 194.69155883789062\n",
            "Total Loss is 197.04527282714844\n",
            "Total Loss is 221.31515502929688\n",
            "Total Loss is 203.28277587890625\n",
            "Total Loss is 215.30661010742188\n",
            "Total Loss is 213.906005859375\n",
            "Total Loss is 200.6748809814453\n",
            "Total Loss is 209.95628356933594\n",
            "Total Loss is 210.78353881835938\n",
            "Total Loss is 196.3190460205078\n",
            "Total Loss is 220.93418884277344\n",
            "Total Loss is 203.68560791015625\n",
            "Total Loss is 214.80172729492188\n",
            "Total Loss is 207.3523406982422\n",
            "Total Loss is 200.7932891845703\n",
            "Total Loss is 207.44036865234375\n",
            "Total Loss is 213.24639892578125\n",
            "Total Loss is 203.91757202148438\n",
            "Total Loss is 217.64053344726562\n",
            "Total Loss is 206.1572723388672\n",
            "Total Loss is 210.4011688232422\n",
            "Total Loss is 217.01307678222656\n",
            "Total Loss is 204.04171752929688\n",
            "Total Loss is 208.8422393798828\n",
            "Total Loss is 207.45361328125\n",
            "Total Loss is 213.058837890625\n",
            "Total Loss is 187.30471801757812\n",
            "Total Loss is 197.57229614257812\n",
            "Total Loss is 215.4755401611328\n",
            "Total Loss is 208.845947265625\n",
            "Total Loss is 212.81396484375\n",
            "Total Loss is 221.09422302246094\n",
            "Total Loss is 206.57623291015625\n",
            "Total Loss is 201.79959106445312\n",
            "Total Loss is 212.6367645263672\n",
            "Total Loss is 201.1720733642578\n",
            "Total Loss is 209.7313995361328\n",
            "Total Loss is 216.40260314941406\n",
            "Total Loss is 214.2637481689453\n",
            "Total Loss is 198.07688903808594\n",
            "Total Loss is 212.50929260253906\n",
            "Total Loss is 212.02978515625\n",
            "Total Loss is 217.90243530273438\n",
            "Total Loss is 204.33050537109375\n",
            "Total Loss is 195.27980041503906\n",
            "Total Loss is 204.5186767578125\n",
            "Total Loss is 205.65887451171875\n",
            "Total Loss is 215.08322143554688\n",
            "Total Loss is 208.34300231933594\n",
            "Total Loss is 221.89170837402344\n",
            "Total Loss is 207.1372528076172\n",
            "Total Loss is 211.77537536621094\n",
            "Total Loss is 199.4326629638672\n",
            "Total Loss is 207.87481689453125\n",
            "Total Loss is 205.23599243164062\n",
            "Total Loss is 217.876220703125\n",
            "Total Loss is 222.569580078125\n",
            "Total Loss is 223.75384521484375\n",
            "Total Loss is 204.3507080078125\n",
            "Total Loss is 205.90524291992188\n",
            "Total Loss is 212.6983184814453\n",
            "Total Loss is 202.89593505859375\n",
            "Total Loss is 196.81680297851562\n",
            "Total Loss is 204.7325439453125\n",
            "Total Loss is 211.94834899902344\n",
            "Total Loss is 209.7428436279297\n",
            "Total Loss is 197.5011444091797\n",
            "Total Loss is 206.08436584472656\n",
            "Total Loss is 216.1078643798828\n",
            "Total Loss is 220.9927215576172\n",
            "Total Loss is 223.6383056640625\n",
            "Total Loss is 186.9027862548828\n",
            "Total Loss is 222.38095092773438\n",
            "Total Loss is 196.62168884277344\n",
            "Total Loss is 209.98516845703125\n",
            "Total Loss is 203.85009765625\n",
            "Total Loss is 203.1862335205078\n",
            "Total Loss is 221.20046997070312\n",
            "Total Loss is 213.84010314941406\n",
            "Total Loss is 198.30995178222656\n",
            "Total Loss is 187.650390625\n",
            "Total Loss is 214.73350524902344\n",
            "Total Loss is 213.91419982910156\n",
            "Total Loss is 208.63238525390625\n",
            "Total Loss is 214.56837463378906\n",
            "Total Loss is 213.91226196289062\n",
            "Total Loss is 207.1583251953125\n",
            "Total Loss is 195.9375457763672\n",
            "Total Loss is 221.1800537109375\n",
            "Total Loss is 208.6125946044922\n",
            "Total Loss is 209.6134490966797\n",
            "Total Loss is 212.0048828125\n",
            "Total Loss is 199.5445556640625\n",
            "Total Loss is 200.56402587890625\n",
            "Total Loss is 210.4931182861328\n",
            "Total Loss is 217.78346252441406\n",
            "Total Loss is 215.0819854736328\n",
            "Total Loss is 206.2872772216797\n",
            "Total Loss is 202.76133728027344\n",
            "Total Loss is 228.8406982421875\n",
            "Total Loss is 210.10267639160156\n",
            "Total Loss is 194.24545288085938\n",
            "Total Loss is 215.54678344726562\n",
            "Total Loss is 208.4793701171875\n",
            "Total Loss is 197.66189575195312\n",
            "Total Loss is 215.46646118164062\n",
            "Total Loss is 203.44384765625\n",
            "Total Loss is 220.16494750976562\n",
            "Total Loss is 213.5292510986328\n",
            "Total Loss is 215.12196350097656\n",
            "Total Loss is 218.68275451660156\n",
            "Total Loss is 201.52699279785156\n",
            "Total Loss is 224.65599060058594\n",
            "Total Loss is 206.60977172851562\n",
            "Total Loss is 212.50543212890625\n"
          ]
        }
      ],
      "source": [
        "gpt_trained = train(3, intent_data, 8, gpt_tok, gpt_pre, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iA9_gHdiEVOK"
      },
      "outputs": [],
      "source": [
        "gpt_trained.save_weights('trained.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HcNb-bbGQHc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FineTuneGPT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}