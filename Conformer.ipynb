{"cells":[{"cell_type":"markdown","metadata":{"id":"asDoEAxqVplF"},"source":["# Conformer Model"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"5QcdNz8AtsXs"}},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":267,"status":"ok","timestamp":1643324031573,"user":{"displayName":"Soren Madsen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02537305481103850793"},"user_tz":480},"id":"Ys76G9SS5sZa"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import Tensor\n","from torch.nn.init import xavier_normal_\n","from torchaudio.functional import rnnt_loss\n","import math"]},{"cell_type":"markdown","metadata":{"id":"gljJpK4CnYlu"},"source":["### Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30_yzO8wnY6k"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Positional Encoding proposed in \"Attention Is All You Need\".\n","    Since transformer contains no recurrence and no convolution, in order for the model to make\n","    use of the order of the sequence, we must add some positional information.\n","    \"Attention Is All You Need\" use sine and cosine functions of different frequencies:\n","        PE_(pos, 2i)    =  sin(pos / power(10000, 2i / d_model))\n","        PE_(pos, 2i+1)  =  cos(pos / power(10000, 2i / d_model))\n","    \"\"\"\n","    def __init__(self, input_dim, max_len = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, input_dim, requires_grad=False)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, input_dim, 2).float() * -(math.log(10000.0) / input_dim))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, length: int):\n","        return self.pe[:, :length]"]},{"cell_type":"markdown","metadata":{"id":"WUROvJjh5hRo"},"source":["## Residual Connection Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLX7n6pp5f30"},"outputs":[],"source":["class ResidualConnectionModule(nn.Module):\n","    \"\"\"\n","    Residual Connection Module.\n","    outputs = (module(inputs) x module_factor + inputs x input_factor)\n","    \"\"\"\n","    def __init__(\n","            self,\n","            module: nn.Module,\n","            module_factor: float = 1.0,\n","            input_factor: float = 1.0,\n","    ):\n","        super(ResidualConnectionModule, self).__init__()\n","        self.module = module\n","        self.module_factor = module_factor\n","        self.input_factor = input_factor\n","\n","    def forward(self, inputs, mask = None):\n","        if mask is None:\n","            return (self.module(inputs) * self.module_factor) + (inputs * self.input_factor)\n","        else:\n","            return (self.module(inputs, mask) * self.module_factor) + (inputs * self.input_factor)"]},{"cell_type":"markdown","metadata":{"id":"0EDMi0vyVo6Y"},"source":["## Multi-Headed Attention Module\n"]},{"cell_type":"markdown","metadata":{"id":"Qu6ElFrFyYO_"},"source":["#### Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHF9rJ8IyXp2"},"outputs":[],"source":["def relative_shift(self, pos_score):\n","    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n","    zeros = pos_score.new_zeros(batch_size, num_heads, seq_length1, 1)\n","    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n","\n","    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n","    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n","\n","    return pos_score\n","\n","def position_scaled_dot_product(q, k, v, pos_enc, mask=None):\n","    d_k = q.size()[-1]\n","    content = torch.matmul(q, k.transpose(-2, -1))          # (seq_len, head_dim) x (head_dim, seq_len)\n","    positional = torch.matmul(q, pos_enc.transpose(-2, -1)) # (seq_len, head_dim) x (head_dim, seq_len)\n","\n","    positional = relative_shift(positional)\n","    attn_logits = (content + positional) / math.sqrt(d_k)   # (seq_len, seq_len) + (seq_len, seq_len)\n","    if mask is not None:\n","        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n","    attention = F.softmax(attn_logits, dim=-1)\n","    values = torch.matmul(attention, v)                     # (seq_len, seq_len) x (seq_len, head_dim)\n","    return values, attention"]},{"cell_type":"markdown","metadata":{"id":"ajwrMh8_ygVa"},"source":["### Relative Multi-Head Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8xqoObCR1rk"},"outputs":[],"source":["class RelativeMultiHeadAttention(nn.Module):\n","  def __init__(self, input_dim, heads):\n","    super(RelativeMultiHeadAttention, self).__init__()\n","    self.input_dim = input_dim\n","    self.heads = heads\n","    self.head_dim = input_dim // heads\n","    assert (input_dim % heads == 0), \"Input dims must be divisible by head num\"\n","\n","    self.qkv_projection = nn.Linear(input_dim, 3 * input_dim, bias=False)\n","    self.pos_projection = nn.Linear(input_dim, input_dim, bias=False)\n","    self.final_linear = nn.Linear(input_dim, input_dim,bias=False)\n","\n","    self.reset_parameters()\n","  \n","  def reset_parameters(self):\n","    nn.init.xavier_uniform_(self.qkv_projection.weight)\n","    nn.init.xavier_uniform_(self.final_linear.weight)\n","\n","  def forward(self, x, positional, mask=None, return_attention=False):\n","    batch_size, seq_len, input_dim = x.size()\n","    qkv = self.qkv_projection(x)\n","    pos = self.pos_projection(positional)\n","\n","    # Separate heads\n","    qkv = qkv.reshape(batch_size, seq_len, self.heads, 3*self.head_dim)\n","    pos = pos.reshape(batch_size, seq_len, self.heads, self.head_dim)\n","    qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n","    pos = pos.permute(0, 2, 1, 3)\n","\n","    q, k, v = qkv.chunk(3, dim=-1)\n","\n","    values, attention = position_scaled_dot_product(q, k, v, pos, mask=mask)\n","    values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n","    values.reshape(batch_size, seq_len, input_dim)\n","\n","    out = self.final_linear(values)\n","\n","    if return_attention:\n","      return out, attention\n","    else:\n","      return out\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HOcX4rtGynja"},"source":["### Module Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiP42Q8eym1s"},"outputs":[],"source":["class MultiheadAttentionModule(nn.Module):\n","  def __init__(self, input_dim, heads = 8, dropout = 0.1):\n","    super(MultiheadAttentionModule, self).__init__()\n","    \n","    self.pe = PositionalEncoding(input_dim),\n","    self.norm = nn.LayerNorm(input_dim),\n","    self.attention = RelativeMultiHeadAttention(input_dim, heads),\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, x, mask = None):\n","    batch_size, seq_len, _ = x.size()\n","    pos_enc = self.pe(seq_len)\n","    pos_enc = pos_enc.repeat(batch_size, 1, 1)\n","\n","    input = self.norm(x)\n","    attn = self.attention(input, pos_enc, mask)\n","\n","    return self.dropout(attn)"]},{"cell_type":"markdown","metadata":{"id":"w39B4r5SVwp4"},"source":["## Feed Forward Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5GB4G_4SXhH"},"outputs":[],"source":["class FeedForward(nn.Module):\n","  def __init__(self, input_dim, forward_expansion, dropout):\n","    self.linear1 = nn.Linear(input_dim, input_dim * forward_expansion)\n","    self.dropout = nn.Dropout(dropout)\n","    self.activation = nn.ReLU(inplace=True)\n","    self.linear2 = nn.Linear(input_dim * forward_expansion, input_dim)\n","  \n","  def forward(self, x):\n","    x = self.linear1(x)\n","    x = self.dropout(x)\n","    x = self.activation(x)\n","    out = self.linear2(x)\n","    return out"]},{"cell_type":"markdown","metadata":{"id":"In2hub1ryANV"},"source":["## Convolution Module"]},{"cell_type":"markdown","metadata":{"id":"a7zKPgtvc-4i"},"source":["### Swish Activation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEgtRHEVc_MZ"},"outputs":[],"source":["class Swish(nn.Module):\n","    def __init__(self):\n","        super(Swish, self).__init__()\n","\n","    def forward(self, inputs):\n","        return inputs * inputs.sigmoid()"]},{"cell_type":"markdown","metadata":{"id":"zFjq7WnJzlED"},"source":["### GLU Activation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cjox6xvzm58"},"outputs":[],"source":["class GLU(nn.Module):\n","    def __init__(self, activation_dim):\n","        super(GLU, self).__init__()\n","        self.activation_dim = activation_dim\n","\n","    def forward(self, inputs):\n","        outputs, gate = inputs.chunk(2, dim=self.activation_dim)\n","        return outputs * gate.sigmoid()"]},{"cell_type":"markdown","metadata":{"id":"jyb-wyolzpEq"},"source":["### Module Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyRDqGFoyAkb"},"outputs":[],"source":["class ConvolutionModule(nn.Module):\n","    def __init__(\n","            self,\n","            input_dim,\n","            kernel_size = 31,\n","            expansion_factor = 2,\n","            dropout = 0.1,\n","    ):\n","        super(ConvolutionModule, self).__init__()\n","        assert (kernel_size - 1) % 2 == 0, \"kernel_size should be a odd number for 'SAME' padding\"\n","        assert expansion_factor == 2, \"Only expansion_factor 2 allowed\"\n","\n","        self.layer_norm  = nn.LayerNorm(input_dim)\n","        self.pointwise1 = nn.nn.Conv1d(\n","            in_channels=input_dim,\n","            out_channels=input_dim * expansion_factor,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True,\n","        )\n","\n","        self.glu = GLU(activation_dim=1)\n","\n","        self.depthwise = nn.nn.Conv1d(\n","            in_channels=input_dim,\n","            out_channels=input_dim,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True,\n","        )\n","\n","        self.batch_norm = nn.BatchNorm1d(input_dim)\n","\n","        self.swish = Swish()\n","\n","        self.pointwise2 = nn.nn.Conv1d(\n","            in_channels=input_dim,\n","            out_channels=input_dim,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=True,\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        x = x.transpose(1,2)\n","        x = self.pointwise1(x)\n","        x = self.glu(x)\n","        x = self.depthwise(x)\n","        x = self.batch_norm(x)\n","        x = self.swish(x)\n","        x = self.pointwise2(x)\n","        x = self.dropout(x)\n","\n","        return x.transpose(1, 2)"]},{"cell_type":"markdown","metadata":{"id":"OFugyBBkXNCX"},"source":["## Conformer Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAxyLVccXOlC"},"outputs":[],"source":["class ConformerBlock(nn.Module):\n","  def __init__(\n","      self,\n","      encoder_dim,\n","      attention_heads = 8,\n","      half_step_residual = True,\n","      ff_expansion = 4,\n","      ff_dropout = 0.1,\n","      attn_dropout = 0.1,\n","      conv_expansion = 2,\n","      conv_dropout = 0.1,\n","      conv_kernel = 31\n","  ):\n","    super(ConformerBlock, self).__init__()\n","    if half_step_residual:\n","      self.ff_residual = 0.5\n","    else:\n","      self.ff_residual = 1\n","\n","    self.sequential([\n","        ResidualConnectionModule(\n","            module=FeedForward(\n","                input_dim=encoder_dim,\n","                forward_expansion=ff_expansion.ff_expansion,\n","                dropout=ff_dropout\n","            ),\n","            module_factor=self.ff_residual\n","        ),\n","        ResidualConnectionModule(\n","            module=MultiheadAttentionModule(\n","                input_dim=encoder_dim,\n","                heads=attention_heads,\n","                dropout=attn_dropout\n","            )\n","        ),\n","        ResidualConnectionModule(\n","            module=ConvolutionModule(\n","                input_dim=encoder_dim,\n","                kernel_size=conv_kernel,\n","                expansion_factor=conv_expansion,\n","                dropout=conv_dropout\n","            )\n","        ),\n","        ResidualConnectionModule(\n","            module=FeedForward(\n","                input_dim=encoder_dim,\n","                forward_expansion=ff_expansion.ff_expansion,\n","                dropout=ff_dropout\n","            ),\n","            module_factor=self.ff_residual\n","        ),\n","        nn.LayerNorm(encoder_dim)\n","    ])\n","    \n","    def forward(self, inputs):\n","      return self.sequential(inputs)"]},{"cell_type":"markdown","source":["## Conformer Encoder"],"metadata":{"id":"dqhaQ-O7EACd"}},{"cell_type":"markdown","source":["### Convolutional Subsampling"],"metadata":{"id":"ZfkJp-k-JK8j"}},{"cell_type":"markdown","source":["Masked 2d Subsampling"],"metadata":{"id":"NW6q9LXmJU48"}},{"cell_type":"code","source":["class MaskConv2d(nn.Module):\n","    r\"\"\"\n","    Masking Convolutional Neural Network\n","    Adds padding to the output of the module based on the given lengths.\n","    This is to ensure that the results of the model do not change when batch sizes change during inference.\n","    Input needs to be in the shape of (batch_size, channel, hidden_dim, seq_len)\n","    Refer to https://github.com/SeanNaren/deepspeech.pytorch/blob/master/model.py\n","    Copyright (c) 2017 Sean Naren\n","    MIT License\n","    Args:\n","        sequential (torch.nn): sequential list of convolution layer\n","    Inputs: inputs, seq_lengths\n","        - **inputs** (torch.FloatTensor): The input of size BxCxHxT\n","        - **seq_lengths** (torch.IntTensor): The actual length of each sequence in the batch\n","    Returns: output, seq_lengths\n","        - **output**: Masked output from the sequential\n","        - **seq_lengths**: Sequence length of output from the sequential\n","    \"\"\"\n","    def __init__(self, sequential: nn.Sequential) -> None:\n","        super(MaskConv2d, self).__init__()\n","        self.sequential = sequential\n","\n","    def forward(self, inputs: Tensor, seq_lengths: Tensor):\n","        output = None\n","\n","        for module in self.sequential:\n","            output = module(inputs)\n","            mask = torch.BoolTensor(output.size()).fill_(0)\n","\n","            if output.is_cuda:\n","                mask = mask.cuda()\n","\n","            seq_lengths = self._get_sequence_lengths(module, seq_lengths)\n","\n","            for idx, length in enumerate(seq_lengths):\n","                length = length.item()\n","\n","                if (mask[idx].size(2) - length) > 0:\n","                    mask[idx].narrow(dim=2, start=length, length=mask[idx].size(2) - length).fill_(1)\n","\n","            output = output.masked_fill(mask, 0)\n","            inputs = output\n","\n","        return output, seq_lengths\n","\n","    def _get_sequence_lengths(self, module: nn.Module, seq_lengths: Tensor) -> Tensor:\n","        r\"\"\"\n","        Calculate convolutional neural network receptive formula\n","        Args:\n","            module (torch.nn.Module): module of CNN\n","            seq_lengths (torch.IntTensor): The actual length of each sequence in the batch\n","        Returns: seq_lengths\n","            - **seq_lengths**: Sequence length of output from the module\n","        \"\"\"\n","        if isinstance(module, nn.Conv2d):\n","            numerator = seq_lengths + 2 * module.padding[1] - module.dilation[1] * (module.kernel_size[1] - 1) - 1\n","            seq_lengths = numerator.float() / float(module.stride[1])\n","            seq_lengths = seq_lengths.int() + 1\n","\n","        elif isinstance(module, nn.MaxPool2d):\n","            seq_lengths >>= 1\n","\n","        return seq_lengths.int()"],"metadata":{"id":"EyUordlUJO8R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convolutional 2d Extractor"],"metadata":{"id":"jgvuZjbvLIaa"}},{"cell_type":"code","source":["class Conv2dExtractor(nn.Module):\n","    r\"\"\"\n","    Provides inteface of convolutional extractor.\n","    Note:\n","        Do not use this class directly, use one of the sub classes.\n","        Define the 'self.conv' class variable.\n","    Inputs: inputs, input_lengths\n","        - **inputs** (batch, time, dim): Tensor containing input vectors\n","        - **input_lengths**: Tensor containing containing sequence lengths\n","    Returns: outputs, output_lengths\n","        - **outputs**: Tensor produced by the convolution\n","        - **output_lengths**: Tensor containing sequence lengths produced by the convolution\n","    \"\"\"\n","    supported_activations = {\n","        'hardtanh': nn.Hardtanh(0, 20, inplace=True),\n","        'relu': nn.ReLU(inplace=True),\n","        'elu': nn.ELU(inplace=True),\n","        'leaky_relu': nn.LeakyReLU(inplace=True),\n","        'gelu': nn.GELU(),\n","        'swish': Swish(),\n","    }\n","\n","    def __init__(self, input_dim: int, activation: str = 'hardtanh') -> None:\n","        super(Conv2dExtractor, self).__init__()\n","        self.input_dim = input_dim\n","        self.activation = Conv2dExtractor.supported_activations[activation]\n","        self.conv = None\n","\n","    def get_output_lengths(self, seq_lengths: torch.Tensor):\n","        assert self.conv is not None, \"self.conv should be defined\"\n","\n","        for module in self.conv:\n","            if isinstance(module, nn.Conv2d):\n","                numerator = seq_lengths + 2 * module.padding[1] - module.dilation[1] * (module.kernel_size[1] - 1) - 1\n","                seq_lengths = numerator.float() / float(module.stride[1])\n","                seq_lengths = seq_lengths.int() + 1\n","\n","            elif isinstance(module, nn.MaxPool2d):\n","                seq_lengths >>= 1\n","\n","        return seq_lengths.int()\n","\n","    def get_output_dim(self):\n","        factor = ((self.input_dim - 1) // 2 - 1) // 2\n","        output_dim = self.out_channels * factor\n","        return output_dim\n","\n","    def forward(self, inputs: Tensor, input_lengths: Tensor):\n","        r\"\"\"\n","        inputs: torch.FloatTensor (batch, time, dimension)\n","        input_lengths: torch.IntTensor (batch)\n","        \"\"\"\n","        outputs, output_lengths = self.conv(inputs.unsqueeze(1).transpose(2, 3), input_lengths)\n","\n","        batch_size, channels, dimension, seq_lengths = outputs.size()\n","        outputs = outputs.permute(0, 3, 1, 2)\n","        outputs = outputs.view(batch_size, seq_lengths, channels * dimension)\n","\n","        return outputs, output_lengths"],"metadata":{"id":"UVDM5pAnLL6q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convolutional 2d Subsample Module"],"metadata":{"id":"PUS5sMWBLeHl"}},{"cell_type":"code","source":["class Conv2dSubsampling(Conv2dExtractor):\n","    r\"\"\"\n","    Convolutional 2D subsampling (to 1/4 length)\n","    Args:\n","        input_dim (int): Dimension of input vector\n","        in_channels (int): Number of channels in the input vector\n","        out_channels (int): Number of channels produced by the convolution\n","        activation (str): Activation function\n","    Inputs: inputs\n","        - **inputs** (batch, time, dim): Tensor containing sequence of inputs\n","        - **input_lengths** (batch): list of sequence input lengths\n","    Returns: outputs, output_lengths\n","        - **outputs** (batch, time, dim): Tensor produced by the convolution\n","        - **output_lengths** (batch): list of sequence output lengths\n","    \"\"\"\n","    def __init__(\n","            self,\n","            input_dim: int,\n","            in_channels: int,\n","            out_channels: int,\n","            activation: str = 'relu',\n","    ) -> None:\n","        super(Conv2dSubsampling, self).__init__(input_dim, activation)\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.conv = MaskConv2d(\n","            nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2),\n","                self.activation,\n","                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2),\n","                self.activation,\n","            )\n","        )\n","\n","    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor):\n","        outputs, output_lengths = super().forward(inputs, input_lengths)\n","\n","        return outputs, output_lengths"],"metadata":{"id":"XhemMSguLiw8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Conformer Encoder Module"],"metadata":{"id":"OYWo8CYIZqCT"}},{"cell_type":"code","source":["class ConformerEncoder(nn.Module):\n","  def __init__(self,\n","      num_layers=16,\n","      input_dim=80,\n","      input_dropout=0.1,\n","      encoder_dim=512,\n","      attention_heads=8,\n","      half_step_residual = True,\n","      ff_expansion = 4,\n","      ff_dropout = 0.1,\n","      attn_dropout = 0.1,\n","      conv_expansion = 2,\n","      conv_dropout = 0.1,\n","      conv_kernel = 32\n","  ):\n","    super(ConformerEncoder, self).__init__()\n","    self.conv_subsample = Conv2dSubsampling(input_dim, in_channels=1, out_channels=encoder_dim)\n","    self.input_projection = nn.Sequential(\n","            nn.Linear(self.conv_subsample.get_output_dim(), encoder_dim),\n","            nn.Dropout(p=input_dropout),\n","        )\n","    self.layers = nn.ModuleList([\n","      ConformerBlock(\n","        encoder_dim=encoder_dim,\n","        attention_heads=attention_heads,\n","        half_step_residual=half_step_residual,\n","        ff_expansion=ff_expansion,\n","        ff_dropout=ff_dropout,\n","        attn_dropout=attn_dropout,\n","        conv_expansion=conv_expansion,\n","        conv_dropout=conv_dropout,\n","        conv_kernel=conv_kernel\n","        ) for _ in range(num_layers)\n","    ])\n","  def forward(self, inputs, input_lengths):\n","    outputs, output_lengths = self.conv_subsample(inputs, input_lengths)\n","    outputs = self.input_projection(outputs)\n","\n","    for layer in self.layers:\n","      outputs = layer(outputs)\n","\n","    return outputs, output_lengths"],"metadata":{"id":"5gMzok3kECSc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conformer Decoder"],"metadata":{"id":"Z3oL_c94Z8iw"}},{"cell_type":"code","source":["class RNNDecoder(nn.Module):\n","  supported_rnns = {\n","    'lstm': nn.LSTM,\n","    'gru': nn.GRU,\n","    'rnn': nn.RNN,\n","  }\n","  def __init__(self,\n","               rnn_type = 'lstm',\n","               num_classes=29,\n","               hidden_state_dim=320,\n","               num_layers=1,\n","\n","               ):\n","    super()\n","    self.embedding = nn.Embedding(num_classes, hidden_state_dim)\n","    rnn_cell = self.supported_rnns[rnn_type.lower()]\n","    self.rnn = rnn_cell(\n","            input_size=hidden_state_dim,\n","            hidden_size=hidden_state_dim,\n","            num_layers=num_layers,\n","            bias=True,\n","            batch_first=True,\n","            dropout=dropout_p,\n","            bidirectional=False,\n","    )\n","    self.out_proj = nn.Linear(hidden_state_dim, output_dim)"],"metadata":{"id":"kpC-8af1bkEr","executionInfo":{"status":"ok","timestamp":1643325553518,"user_tz":480,"elapsed":198,"user":{"displayName":"Soren Madsen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02537305481103850793"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_WfXvNZa1fg_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["gljJpK4CnYlu","WUROvJjh5hRo","0EDMi0vyVo6Y","Qu6ElFrFyYO_","ajwrMh8_ygVa","HOcX4rtGynja","a7zKPgtvc-4i","zFjq7WnJzlED"],"name":"Conformer.ipynb","provenance":[],"authorship_tag":"ABX9TyNvppQ6eC7MblzoUWxHzcuu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}