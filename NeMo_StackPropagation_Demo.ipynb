{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VJsWecfLRyqI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyaudio as pa\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7gycYReDRyqJ",
    "outputId": "e577529b-c2f7-4652-9cb5-6b017894771d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-23 12:47:44 optimizers:52] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-01-23 12:47:49 experimental:27] Module <function get_argmin_mat at 0x148b3b160> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-23 12:47:49 experimental:27] Module <function getMultiScaleCosAffinityMatrix at 0x148b3b1f0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-23 12:47:49 experimental:27] Module <function parse_scale_configs at 0x148b3bd30> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-01-23 12:47:49 experimental:27] Module <function get_embs_and_timestamps at 0x148b3bdc0> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "v7Jqlq3mRyqK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0rc0\n"
     ]
    }
   ],
   "source": [
    "# sample rate, Hz\n",
    "SAMPLE_RATE = 16000\n",
    "import torch\n",
    "print(nemo.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deo5BR5ZRyqK"
   },
   "source": [
    "## Restore the model from NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDBizFPSRyqL",
    "outputId": "76794ebe-9d87-4a1c-f3ea-7e39237d93cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-23 12:47:50 cloud:56] Found existing object /Users/sorenmadsen/.cache/torch/NeMo/NeMo_1.6.0rc0/QuartzNet15x5Base-En/2b066be39e9294d7100fb176ec817722/QuartzNet15x5Base-En.nemo.\n",
      "[NeMo I 2022-01-23 12:47:50 cloud:62] Re-using file from: /Users/sorenmadsen/.cache/torch/NeMo/NeMo_1.6.0rc0/QuartzNet15x5Base-En/2b066be39e9294d7100fb176ec817722/QuartzNet15x5Base-En.nemo\n",
      "[NeMo I 2022-01-23 12:47:50 common:729] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2022-01-23 12:47:52 features:264] PADDING: 16\n",
      "[NeMo I 2022-01-23 12:47:52 features:281] STFT using torch\n",
      "[NeMo I 2022-01-23 12:47:53 save_restore_connector:149] Model EncDecCTCModel was successfully restored from /Users/sorenmadsen/.cache/torch/NeMo/NeMo_1.6.0rc0/QuartzNet15x5Base-En/2b066be39e9294d7100fb176ec817722/QuartzNet15x5Base-En.nemo.\n"
     ]
    }
   ],
   "source": [
    "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"QuartzNet15x5Base-En\", strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3IE2Zt5RyqM"
   },
   "source": [
    "## Observing the config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JVpw5wdhRyqM"
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDALXrg9RyqM",
    "outputId": "e5119f06-8d34-45b7-89fb-027868bc201a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor:\n",
      "  _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
      "  normalize: per_feature\n",
      "  window_size: 0.02\n",
      "  sample_rate: 16000\n",
      "  window_stride: 0.01\n",
      "  window: hann\n",
      "  features: 64\n",
      "  n_fft: 512\n",
      "  frame_splicing: 1\n",
      "  dither: 1.0e-05\n",
      "  stft_conv: false\n",
      "spec_augment:\n",
      "  _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
      "  rect_freq: 50\n",
      "  rect_masks: 5\n",
      "  rect_time: 120\n",
      "encoder:\n",
      "  _target_: nemo.collections.asr.modules.ConvASREncoder\n",
      "  feat_in: 64\n",
      "  activation: relu\n",
      "  conv_mask: true\n",
      "  jasper:\n",
      "  - filters: 256\n",
      "    repeat: 1\n",
      "    kernel:\n",
      "    - 33\n",
      "    stride:\n",
      "    - 2\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: false\n",
      "    separable: true\n",
      "  - filters: 256\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 33\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 256\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 33\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 256\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 33\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 256\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 39\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 256\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 39\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 256\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 39\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 51\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 51\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 51\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 63\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 63\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 63\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 75\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 75\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 5\n",
      "    kernel:\n",
      "    - 75\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: true\n",
      "    separable: true\n",
      "  - filters: 512\n",
      "    repeat: 1\n",
      "    kernel:\n",
      "    - 87\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 2\n",
      "    dropout: 0.0\n",
      "    residual: false\n",
      "    separable: true\n",
      "  - filters: 1024\n",
      "    repeat: 1\n",
      "    kernel:\n",
      "    - 1\n",
      "    stride:\n",
      "    - 1\n",
      "    dilation:\n",
      "    - 1\n",
      "    dropout: 0.0\n",
      "    residual: false\n",
      "decoder:\n",
      "  _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
      "  feat_in: 1024\n",
      "  num_classes: 28\n",
      "  vocabulary:\n",
      "  - ' '\n",
      "  - a\n",
      "  - b\n",
      "  - c\n",
      "  - d\n",
      "  - e\n",
      "  - f\n",
      "  - g\n",
      "  - h\n",
      "  - i\n",
      "  - j\n",
      "  - k\n",
      "  - l\n",
      "  - m\n",
      "  - 'n'\n",
      "  - o\n",
      "  - p\n",
      "  - q\n",
      "  - r\n",
      "  - s\n",
      "  - t\n",
      "  - u\n",
      "  - v\n",
      "  - w\n",
      "  - x\n",
      "  - 'y'\n",
      "  - z\n",
      "  - ''''\n",
      "optim:\n",
      "  name: novograd\n",
      "  lr: 0.01\n",
      "  betas:\n",
      "  - 0.8\n",
      "  - 0.5\n",
      "  weight_decay: 0.001\n",
      "target: nemo.collections.asr.models.ctc_models.EncDecCTCModel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preserve a copy of the full config\n",
    "cfg = copy.deepcopy(asr_model._cfg)\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH2-qRp0RyqN"
   },
   "source": [
    "### Modify preprocessor parameters for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YfmgxC8HRyqN"
   },
   "outputs": [],
   "source": [
    "# Make config overwrite-able\n",
    "OmegaConf.set_struct(cfg.preprocessor, False)\n",
    "\n",
    "# some changes for streaming scenario\n",
    "cfg.preprocessor.dither = 0.0\n",
    "cfg.preprocessor.pad_to = 0\n",
    "\n",
    "# spectrogram normalization constants\n",
    "normalization = {}\n",
    "normalization['fixed_mean'] = [\n",
    "     -14.95827016, -12.71798736, -11.76067913, -10.83311182,\n",
    "     -10.6746914,  -10.15163465, -10.05378331, -9.53918999,\n",
    "     -9.41858904,  -9.23382904,  -9.46470918,  -9.56037,\n",
    "     -9.57434245,  -9.47498732,  -9.7635205,   -10.08113074,\n",
    "     -10.05454561, -9.81112681,  -9.68673603,  -9.83652977,\n",
    "     -9.90046248,  -9.85404766,  -9.92560366,  -9.95440354,\n",
    "     -10.17162966, -9.90102482,  -9.47471025,  -9.54416855,\n",
    "     -10.07109475, -9.98249912,  -9.74359465,  -9.55632283,\n",
    "     -9.23399915,  -9.36487649,  -9.81791084,  -9.56799225,\n",
    "     -9.70630899,  -9.85148006,  -9.8594418,   -10.01378735,\n",
    "     -9.98505315,  -9.62016094,  -10.342285,   -10.41070709,\n",
    "     -10.10687659, -10.14536695, -10.30828702, -10.23542833,\n",
    "     -10.88546868, -11.31723646, -11.46087382, -11.54877829,\n",
    "     -11.62400934, -11.92190509, -12.14063815, -11.65130117,\n",
    "     -11.58308531, -12.22214663, -12.42927197, -12.58039805,\n",
    "     -13.10098969, -13.14345864, -13.31835645, -14.47345634]\n",
    "normalization['fixed_std'] = [\n",
    "     3.81402054, 4.12647781, 4.05007065, 3.87790987,\n",
    "     3.74721178, 3.68377423, 3.69344,    3.54001005,\n",
    "     3.59530412, 3.63752368, 3.62826417, 3.56488469,\n",
    "     3.53740577, 3.68313898, 3.67138151, 3.55707266,\n",
    "     3.54919572, 3.55721289, 3.56723346, 3.46029304,\n",
    "     3.44119672, 3.49030548, 3.39328435, 3.28244406,\n",
    "     3.28001423, 3.26744937, 3.46692348, 3.35378948,\n",
    "     2.96330901, 2.97663111, 3.04575148, 2.89717604,\n",
    "     2.95659301, 2.90181116, 2.7111687,  2.93041291,\n",
    "     2.86647897, 2.73473181, 2.71495654, 2.75543763,\n",
    "     2.79174615, 2.96076456, 2.57376336, 2.68789782,\n",
    "     2.90930817, 2.90412004, 2.76187531, 2.89905006,\n",
    "     2.65896173, 2.81032176, 2.87769857, 2.84665271,\n",
    "     2.80863137, 2.80707634, 2.83752184, 3.01914511,\n",
    "     2.92046439, 2.78461139, 2.90034605, 2.94599508,\n",
    "     2.99099718, 3.0167554,  3.04649716, 2.94116777]\n",
    "\n",
    "cfg.preprocessor.normalize = normalization\n",
    "\n",
    "# Disable config overwriting\n",
    "OmegaConf.set_struct(cfg.preprocessor, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnDcHqDLRyqP"
   },
   "source": [
    "## Setup preprocessor with these settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiuSVTzjRyqQ",
    "outputId": "f99eb50b-1f3e-4829-b660-4692130bc1b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-23 12:47:53 features:264] PADDING: 0\n",
      "[NeMo I 2022-01-23 12:47:53 features:281] STFT using torch\n"
     ]
    }
   ],
   "source": [
    "asr_model.preprocessor = asr_model.from_config_dict(cfg.preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZIeq3UAiRyqQ"
   },
   "outputs": [],
   "source": [
    "# Set model to inference mode\n",
    "asr_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5-PzamH6RyqQ"
   },
   "outputs": [],
   "source": [
    "asr_model = asr_model.to(asr_model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9qYScm2RyqR"
   },
   "source": [
    "## Setting up data for Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VSggJyvuRyqR"
   },
   "outputs": [],
   "source": [
    "from nemo.core.classes import IterableDataset\n",
    "from nemo.core.neural_types import NeuralType, AudioSignal, LengthsType\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LBvE_LdaRyqR"
   },
   "outputs": [],
   "source": [
    "# simple data layer to pass audio signal\n",
    "class AudioDataLayer(IterableDataset):\n",
    "    @property\n",
    "    def output_types(self):\n",
    "        return {\n",
    "            'audio_signal': NeuralType(('B', 'T'), AudioSignal(freq=self._sample_rate)),\n",
    "            'a_sig_length': NeuralType(tuple('B'), LengthsType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, sample_rate):\n",
    "        super().__init__()\n",
    "        self._sample_rate = sample_rate\n",
    "        self.output = True\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if not self.output:\n",
    "            raise StopIteration\n",
    "        self.output = False\n",
    "        return torch.as_tensor(self.signal, dtype=torch.float32), \\\n",
    "               torch.as_tensor(self.signal_shape, dtype=torch.int64)\n",
    "        \n",
    "    def set_signal(self, signal):\n",
    "        self.signal = signal.astype(np.float32)/32768.\n",
    "        self.signal_shape = self.signal.size\n",
    "        self.output = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GGDCbpS-RyqS"
   },
   "outputs": [],
   "source": [
    "data_layer = AudioDataLayer(sample_rate=cfg.preprocessor.sample_rate)\n",
    "data_loader = DataLoader(data_layer, batch_size=1, collate_fn=data_layer.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "WlGSCE2YRyqS"
   },
   "outputs": [],
   "source": [
    "# inference method for audio signal (single instance)\n",
    "def infer_signal(model, signal):\n",
    "    data_layer.set_signal(signal)\n",
    "    batch = next(iter(data_loader))\n",
    "    audio_signal, audio_signal_len = batch\n",
    "    audio_signal, audio_signal_len = audio_signal.to(asr_model.device), audio_signal_len.to(asr_model.device)\n",
    "    log_probs, encoded_len, predictions = model.forward(\n",
    "        input_signal=audio_signal, input_signal_length=audio_signal_len\n",
    "    )\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3Z3OVlTYRyqS"
   },
   "outputs": [],
   "source": [
    "# class for streaming frame-based ASR\n",
    "# 1) use reset() method to reset FrameASR's state\n",
    "# 2) call transcribe(frame) to do ASR on\n",
    "#    contiguous signal's frames\n",
    "class FrameASR:\n",
    "    \n",
    "    def __init__(self, model_definition,\n",
    "                 frame_len=2, frame_overlap=2.5, \n",
    "                 offset=10):\n",
    "        '''\n",
    "        Args:\n",
    "          frame_len: frame's duration, seconds\n",
    "          frame_overlap: duration of overlaps before and after current frame, seconds\n",
    "          offset: number of symbols to drop for smooth streaming\n",
    "        '''\n",
    "        self.vocab = list(model_definition['labels'])\n",
    "        self.vocab.append('_')\n",
    "        \n",
    "        self.sr = model_definition['sample_rate']\n",
    "        self.frame_len = frame_len\n",
    "        self.n_frame_len = int(frame_len * self.sr)\n",
    "        self.frame_overlap = frame_overlap\n",
    "        self.n_frame_overlap = int(frame_overlap * self.sr)\n",
    "        timestep_duration = model_definition['AudioToMelSpectrogramPreprocessor']['window_stride']\n",
    "        for block in model_definition['JasperEncoder']['jasper']:\n",
    "            timestep_duration *= block['stride'][0] ** block['repeat']\n",
    "        self.n_timesteps_overlap = int(frame_overlap / timestep_duration) - 2\n",
    "        self.buffer = np.zeros(shape=2*self.n_frame_overlap + self.n_frame_len,\n",
    "                               dtype=np.float32)\n",
    "        self.offset = offset\n",
    "        self.reset()\n",
    "        \n",
    "    def _decode(self, frame, offset=0):\n",
    "        assert len(frame)==self.n_frame_len\n",
    "        self.buffer[:-self.n_frame_len] = self.buffer[self.n_frame_len:]\n",
    "        self.buffer[-self.n_frame_len:] = frame\n",
    "        logits = infer_signal(asr_model, self.buffer).cpu().numpy()[0]\n",
    "        print(logits.shape)\n",
    "        decoded = self._greedy_decoder(\n",
    "            logits[self.n_timesteps_overlap:-self.n_timesteps_overlap], \n",
    "            self.vocab\n",
    "        )\n",
    "        return decoded[:len(decoded)-offset]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def transcribe(self, frame=None, merge=True):\n",
    "        if frame is None:\n",
    "            frame = np.zeros(shape=self.n_frame_len, dtype=np.float32)\n",
    "        if len(frame) < self.n_frame_len:\n",
    "            frame = np.pad(frame, [0, self.n_frame_len - len(frame)], 'constant')\n",
    "        unmerged = self._decode(frame, self.offset)\n",
    "        if not merge:\n",
    "            return unmerged\n",
    "        return self.greedy_merge(unmerged)\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset frame_history and decoder's state\n",
    "        '''\n",
    "        self.buffer=np.zeros(shape=self.buffer.shape, dtype=np.float32)\n",
    "        self.prev_char = ''\n",
    "\n",
    "    @staticmethod\n",
    "    def _greedy_decoder(logits, vocab):\n",
    "        s = ''\n",
    "        print(logits.shape)\n",
    "        for i in range(logits.shape[0]):\n",
    "            s += vocab[np.argmax(logits[i])]\n",
    "        return s\n",
    "\n",
    "    def greedy_merge(self, s):\n",
    "        s_merged = ''\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            if s[i] != self.prev_char:\n",
    "                self.prev_char = s[i]\n",
    "                if self.prev_char != '_':\n",
    "                    s_merged += self.prev_char\n",
    "        return s_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class ModelManager(nn.Module):\n",
    "\n",
    "    def __init__(self, args, num_word, num_slot, num_intent):\n",
    "        super(ModelManager, self).__init__()\n",
    "\n",
    "        self.__num_word = num_word\n",
    "        self.__num_slot = num_slot\n",
    "        self.__num_intent = num_intent\n",
    "        self.__args = args\n",
    "\n",
    "        # Initialize an embedding object.\n",
    "        self.__embedding = EmbeddingCollection(\n",
    "            self.__num_word,\n",
    "            self.__args.word_embedding_dim\n",
    "        )\n",
    "\n",
    "        # Initialize an LSTM Encoder object.\n",
    "        self.__encoder = LSTMEncoder(\n",
    "            self.__args.word_embedding_dim,\n",
    "            self.__args.encoder_hidden_dim,\n",
    "            self.__args.dropout_rate\n",
    "        )\n",
    "\n",
    "        # Initialize an self-attention layer.\n",
    "        self.__attention = SelfAttention(\n",
    "            self.__args.word_embedding_dim,\n",
    "            self.__args.attention_hidden_dim,\n",
    "            self.__args.attention_output_dim,\n",
    "            self.__args.dropout_rate\n",
    "        )\n",
    "\n",
    "        # Initialize an Decoder object for intent.\n",
    "        self.__intent_decoder = LSTMDecoder(\n",
    "            self.__args.encoder_hidden_dim + self.__args.attention_output_dim,\n",
    "            self.__args.intent_decoder_hidden_dim,\n",
    "            self.__num_intent, self.__args.dropout_rate,\n",
    "            embedding_dim=self.__args.intent_embedding_dim\n",
    "        )\n",
    "        # Initialize an Decoder object for slot.\n",
    "        self.__slot_decoder = LSTMDecoder(\n",
    "            self.__args.encoder_hidden_dim + self.__args.attention_output_dim,\n",
    "            self.__args.slot_decoder_hidden_dim,\n",
    "            self.__num_slot, self.__args.dropout_rate,\n",
    "            embedding_dim=self.__args.slot_embedding_dim,\n",
    "            extra_dim=self.__num_intent\n",
    "        )\n",
    "\n",
    "        # One-hot encoding for augment data feed. \n",
    "        self.__intent_embedding = nn.Embedding(\n",
    "            self.__num_intent, self.__num_intent\n",
    "        )\n",
    "        self.__intent_embedding.weight.data = torch.eye(self.__num_intent)\n",
    "        self.__intent_embedding.weight.requires_grad = False\n",
    "\n",
    "    def show_summary(self):\n",
    "        \"\"\"\n",
    "        print the abstract of the defined model.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Model parameters are listed as follows:\\n')\n",
    "\n",
    "        print('\\tnumber of word:                            {};'.format(self.__num_word))\n",
    "        print('\\tnumber of slot:                            {};'.format(self.__num_slot))\n",
    "        print('\\tnumber of intent:\t\t\t\t\t\t    {};'.format(self.__num_intent))\n",
    "        print('\\tword embedding dimension:\t\t\t\t    {};'.format(self.__args.word_embedding_dim))\n",
    "        print('\\tencoder hidden dimension:\t\t\t\t    {};'.format(self.__args.encoder_hidden_dim))\n",
    "        print('\\tdimension of intent embedding:\t\t    \t{};'.format(self.__args.intent_embedding_dim))\n",
    "        print('\\tdimension of slot embedding:\t\t\t    {};'.format(self.__args.slot_embedding_dim))\n",
    "        print('\\tdimension of slot decoder hidden:  \t    {};'.format(self.__args.slot_decoder_hidden_dim))\n",
    "        print('\\tdimension of intent decoder hidden:        {};'.format(self.__args.intent_decoder_hidden_dim))\n",
    "        print('\\thidden dimension of self-attention:        {};'.format(self.__args.attention_hidden_dim))\n",
    "        print('\\toutput dimension of self-attention:        {};'.format(self.__args.attention_output_dim))\n",
    "\n",
    "        print('\\nEnd of parameters show. Now training begins.\\n\\n')\n",
    "\n",
    "    def forward(self, text, seq_lens, n_predicts=None, forced_slot=None, forced_intent=None):\n",
    "        word_tensor, _ = self.__embedding(text)\n",
    "\n",
    "        lstm_hiddens = self.__encoder(word_tensor, seq_lens)\n",
    "        # transformer_hiddens = self.__transformer(pos_tensor, seq_lens)\n",
    "        attention_hiddens = self.__attention(word_tensor, seq_lens)\n",
    "        hiddens = torch.cat([attention_hiddens, lstm_hiddens], dim=1)\n",
    "\n",
    "        pred_intent = self.__intent_decoder(\n",
    "            hiddens, seq_lens,\n",
    "            forced_input=forced_intent\n",
    "        )\n",
    "\n",
    "        if not self.__args.differentiable:\n",
    "            _, idx_intent = pred_intent.topk(1, dim=-1)\n",
    "            feed_intent = self.__intent_embedding(idx_intent.squeeze(1))\n",
    "        else:\n",
    "            feed_intent = pred_intent\n",
    "\n",
    "        pred_slot = self.__slot_decoder(\n",
    "            hiddens, seq_lens,\n",
    "            forced_input=forced_slot,\n",
    "            extra_input=feed_intent\n",
    "        )\n",
    "\n",
    "        if n_predicts is None:\n",
    "            return F.log_softmax(pred_slot, dim=1), F.log_softmax(pred_intent, dim=1)\n",
    "        else:\n",
    "            _, slot_index = pred_slot.topk(n_predicts, dim=1)\n",
    "            _, intent_index = pred_intent.topk(n_predicts, dim=1)\n",
    "\n",
    "            return slot_index.cpu().data.numpy().tolist(), intent_index.cpu().data.numpy().tolist()\n",
    "\n",
    "    def golden_intent_predict_slot(self, text, seq_lens, golden_intent, n_predicts=1):\n",
    "        word_tensor, _ = self.__embedding(text)\n",
    "        embed_intent = self.__intent_embedding(golden_intent)\n",
    "\n",
    "        lstm_hiddens = self.__encoder(word_tensor, seq_lens)\n",
    "        attention_hiddens = self.__attention(word_tensor, seq_lens)\n",
    "        hiddens = torch.cat([attention_hiddens, lstm_hiddens], dim=1)\n",
    "\n",
    "        pred_slot = self.__slot_decoder(\n",
    "            hiddens, seq_lens, extra_input=embed_intent\n",
    "        )\n",
    "        _, slot_index = pred_slot.topk(n_predicts, dim=-1)\n",
    "\n",
    "        # Just predict single slot value.\n",
    "        return slot_index.cpu().data.numpy().tolist()\n",
    "\n",
    "\n",
    "class EmbeddingCollection(nn.Module):\n",
    "    \"\"\"\n",
    "    Provide word vector and position vector encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, max_len=5000):\n",
    "        super(EmbeddingCollection, self).__init__()\n",
    "\n",
    "        self.__input_dim = input_dim\n",
    "        # Here embedding_dim must be an even embedding.\n",
    "        self.__embedding_dim = embedding_dim\n",
    "        self.__max_len = max_len\n",
    "\n",
    "        # Word vector encoder.\n",
    "        self.__embedding_layer = nn.Embedding(\n",
    "            self.__input_dim, self.__embedding_dim\n",
    "        )\n",
    "\n",
    "        # Position vector encoder.\n",
    "        # self.__position_layer = torch.zeros(self.__max_len, self.__embedding_dim)\n",
    "        # position = torch.arange(0, self.__max_len).unsqueeze(1)\n",
    "        # div_term = torch.exp(torch.arange(0, self.__embedding_dim, 2) *\n",
    "        #                      (-math.log(10000.0) / self.__embedding_dim))\n",
    "\n",
    "        # Sine wave curve design.\n",
    "        # self.__position_layer[:, 0::2] = torch.sin(position * div_term)\n",
    "        # self.__position_layer[:, 1::2] = torch.cos(position * div_term)\n",
    "        #\n",
    "        # self.__position_layer = self.__position_layer.unsqueeze(0)\n",
    "        # self.register_buffer('pe', self.__position_layer)\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        # Get word vector encoding.\n",
    "        embedding_x = self.__embedding_layer(input_x)\n",
    "\n",
    "        # Get position encoding.\n",
    "        # position_x = Variable(self.pe[:, :input_x.size(1)], requires_grad=False)\n",
    "\n",
    "        # Board-casting principle.\n",
    "        return embedding_x, embedding_x\n",
    "\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder structure based on bidirectional LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout_rate):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "\n",
    "        # Parameter recording.\n",
    "        self.__embedding_dim = embedding_dim\n",
    "        self.__hidden_dim = hidden_dim // 2\n",
    "        self.__dropout_rate = dropout_rate\n",
    "\n",
    "        # Network attributes.\n",
    "        self.__dropout_layer = nn.Dropout(self.__dropout_rate)\n",
    "        self.__lstm_layer = nn.LSTM(\n",
    "            input_size=self.__embedding_dim,\n",
    "            hidden_size=self.__hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=self.__dropout_rate,\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_text, seq_lens):\n",
    "        \"\"\" Forward process for LSTM Encoder.\n",
    "        (batch_size, max_sent_len)\n",
    "        -> (batch_size, max_sent_len, word_dim)\n",
    "        -> (batch_size, max_sent_len, hidden_dim)\n",
    "        -> (total_word_num, hidden_dim)\n",
    "        :param embedded_text: padded and embedded input text.\n",
    "        :param seq_lens: is the length of original input text.\n",
    "        :return: is encoded word hidden vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Padded_text should be instance of LongTensor.\n",
    "        dropout_text = self.__dropout_layer(embedded_text)\n",
    "\n",
    "        # Pack and Pad process for input of variable length.\n",
    "        packed_text = pack_padded_sequence(dropout_text, seq_lens, batch_first=True)\n",
    "        lstm_hiddens, (h_last, c_last) = self.__lstm_layer(packed_text)\n",
    "        padded_hiddens, _ = pad_packed_sequence(lstm_hiddens, batch_first=True)\n",
    "\n",
    "        return torch.cat([padded_hiddens[i][:seq_lens[i], :] for i in range(0, len(seq_lens))], dim=0)\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder structure based on unidirectional LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate, embedding_dim=None, extra_dim=None):\n",
    "        \"\"\" Construction function for Decoder.\n",
    "        :param input_dim: input dimension of Decoder. In fact, it's encoder hidden size.\n",
    "        :param hidden_dim: hidden dimension of iterative LSTM.\n",
    "        :param output_dim: output dimension of Decoder. In fact, it's total number of intent or slot.\n",
    "        :param dropout_rate: dropout rate of network which is only useful for embedding.\n",
    "        :param embedding_dim: if it's not None, the input and output are relevant.\n",
    "        :param extra_dim: if it's not None, the decoder receives information tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        self.__input_dim = input_dim\n",
    "        self.__hidden_dim = hidden_dim\n",
    "        self.__output_dim = output_dim\n",
    "        self.__dropout_rate = dropout_rate\n",
    "        self.__embedding_dim = embedding_dim\n",
    "        self.__extra_dim = extra_dim\n",
    "\n",
    "        # If embedding_dim is not None, the output and input\n",
    "        # of this structure is relevant.\n",
    "        if self.__embedding_dim is not None:\n",
    "            self.__embedding_layer = nn.Embedding(output_dim, embedding_dim)\n",
    "            self.__init_tensor = nn.Parameter(\n",
    "                torch.randn(1, self.__embedding_dim),\n",
    "                requires_grad=True\n",
    "            )\n",
    "\n",
    "        # Make sure the input dimension of iterative LSTM.\n",
    "        if self.__extra_dim is not None and self.__embedding_dim is not None:\n",
    "            lstm_input_dim = self.__input_dim + self.__extra_dim + self.__embedding_dim\n",
    "        elif self.__extra_dim is not None:\n",
    "            lstm_input_dim = self.__input_dim + self.__extra_dim\n",
    "        elif self.__embedding_dim is not None:\n",
    "            lstm_input_dim = self.__input_dim + self.__embedding_dim\n",
    "        else:\n",
    "            lstm_input_dim = self.__input_dim\n",
    "\n",
    "        # Network parameter definition.\n",
    "        self.__dropout_layer = nn.Dropout(self.__dropout_rate)\n",
    "        self.__lstm_layer = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=self.__hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=self.__dropout_rate,\n",
    "            num_layers=1\n",
    "        )\n",
    "        self.__linear_layer = nn.Linear(\n",
    "            self.__hidden_dim,\n",
    "            self.__output_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, encoded_hiddens, seq_lens, forced_input=None, extra_input=None):\n",
    "        \"\"\" Forward process for decoder.\n",
    "        :param encoded_hiddens: is encoded hidden tensors produced by encoder.\n",
    "        :param seq_lens: is a list containing lengths of sentence.\n",
    "        :param forced_input: is truth values of label, provided by teacher forcing.\n",
    "        :param extra_input: comes from another decoder as information tensor.\n",
    "        :return: is distribution of prediction labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate information tensor if possible.\n",
    "        if extra_input is not None:\n",
    "            input_tensor = torch.cat([encoded_hiddens, extra_input], dim=1)\n",
    "        else:\n",
    "            input_tensor = encoded_hiddens\n",
    "\n",
    "        output_tensor_list, sent_start_pos = [], 0\n",
    "        if self.__embedding_dim is None or forced_input is not None:\n",
    "\n",
    "            for sent_i in range(0, len(seq_lens)):\n",
    "                sent_end_pos = sent_start_pos + seq_lens[sent_i]\n",
    "\n",
    "                # Segment input hidden tensors.\n",
    "                seg_hiddens = input_tensor[sent_start_pos: sent_end_pos, :]\n",
    "\n",
    "                if self.__embedding_dim is not None and forced_input is not None:\n",
    "                    if seq_lens[sent_i] > 1:\n",
    "                        seg_forced_input = forced_input[sent_start_pos: sent_end_pos]\n",
    "                        seg_forced_tensor = self.__embedding_layer(seg_forced_input).view(seq_lens[sent_i], -1)\n",
    "                        seg_prev_tensor = torch.cat([self.__init_tensor, seg_forced_tensor[:-1, :]], dim=0)\n",
    "                    else:\n",
    "                        seg_prev_tensor = self.__init_tensor\n",
    "\n",
    "                    # Concatenate forced target tensor.\n",
    "                    combined_input = torch.cat([seg_hiddens, seg_prev_tensor], dim=1)\n",
    "                else:\n",
    "                    combined_input = seg_hiddens\n",
    "                dropout_input = self.__dropout_layer(combined_input)\n",
    "\n",
    "                lstm_out, _ = self.__lstm_layer(dropout_input.view(1, seq_lens[sent_i], -1))\n",
    "                linear_out = self.__linear_layer(lstm_out.view(seq_lens[sent_i], -1))\n",
    "\n",
    "                output_tensor_list.append(linear_out)\n",
    "                sent_start_pos = sent_end_pos\n",
    "        else:\n",
    "            for sent_i in range(0, len(seq_lens)):\n",
    "                prev_tensor = self.__init_tensor\n",
    "\n",
    "                # It's necessary to remember h and c state\n",
    "                # when output prediction every single step.\n",
    "                last_h, last_c = None, None\n",
    "\n",
    "                sent_end_pos = sent_start_pos + seq_lens[sent_i]\n",
    "                for word_i in range(sent_start_pos, sent_end_pos):\n",
    "                    seg_input = input_tensor[[word_i], :]\n",
    "                    combined_input = torch.cat([seg_input, prev_tensor], dim=1)\n",
    "                    dropout_input = self.__dropout_layer(combined_input).view(1, 1, -1)\n",
    "\n",
    "                    if last_h is None and last_c is None:\n",
    "                        lstm_out, (last_h, last_c) = self.__lstm_layer(dropout_input)\n",
    "                    else:\n",
    "                        lstm_out, (last_h, last_c) = self.__lstm_layer(dropout_input, (last_h, last_c))\n",
    "\n",
    "                    lstm_out = self.__linear_layer(lstm_out.view(1, -1))\n",
    "                    output_tensor_list.append(lstm_out)\n",
    "\n",
    "                    _, index = lstm_out.topk(1, dim=1)\n",
    "                    prev_tensor = self.__embedding_layer(index).view(1, -1)\n",
    "                sent_start_pos = sent_end_pos\n",
    "\n",
    "        return torch.cat(output_tensor_list, dim=0)\n",
    "\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism based on Query-Key-Value architecture. And\n",
    "    especially, when query == key == value, it's self-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, key_dim, value_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(QKVAttention, self).__init__()\n",
    "\n",
    "        # Record hyper-parameters.\n",
    "        self.__query_dim = query_dim\n",
    "        self.__key_dim = key_dim\n",
    "        self.__value_dim = value_dim\n",
    "        self.__hidden_dim = hidden_dim\n",
    "        self.__output_dim = output_dim\n",
    "        self.__dropout_rate = dropout_rate\n",
    "\n",
    "        # Declare network structures.\n",
    "        self.__query_layer = nn.Linear(self.__query_dim, self.__hidden_dim)\n",
    "        self.__key_layer = nn.Linear(self.__key_dim, self.__hidden_dim)\n",
    "        self.__value_layer = nn.Linear(self.__value_dim, self.__output_dim)\n",
    "        self.__dropout_layer = nn.Dropout(p=self.__dropout_rate)\n",
    "\n",
    "    def forward(self, input_query, input_key, input_value):\n",
    "        \"\"\" The forward propagation of attention.\n",
    "        Here we require the first dimension of input key\n",
    "        and value are equal.\n",
    "        :param input_query: is query tensor, (n, d_q)\n",
    "        :param input_key:  is key tensor, (m, d_k)\n",
    "        :param input_value:  is value tensor, (m, d_v)\n",
    "        :return: attention based tensor, (n, d_h)\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear transform to fine-tune dimension.\n",
    "        linear_query = self.__query_layer(input_query)\n",
    "        linear_key = self.__key_layer(input_key)\n",
    "        linear_value = self.__value_layer(input_value)\n",
    "\n",
    "        score_tensor = F.softmax(torch.matmul(\n",
    "            linear_query,\n",
    "            linear_key.transpose(-2, -1)\n",
    "        ) / math.sqrt(self.__hidden_dim), dim=-1)\n",
    "        forced_tensor = torch.matmul(score_tensor, linear_value)\n",
    "        forced_tensor = self.__dropout_layer(forced_tensor)\n",
    "\n",
    "        return forced_tensor\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        # Record parameters.\n",
    "        self.__input_dim = input_dim\n",
    "        self.__hidden_dim = hidden_dim\n",
    "        self.__output_dim = output_dim\n",
    "        self.__dropout_rate = dropout_rate\n",
    "\n",
    "        # Record network parameters.\n",
    "        self.__dropout_layer = nn.Dropout(self.__dropout_rate)\n",
    "        self.__attention_layer = QKVAttention(\n",
    "            self.__input_dim, self.__input_dim, self.__input_dim,\n",
    "            self.__hidden_dim, self.__output_dim, self.__dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, input_x, seq_lens):\n",
    "        dropout_x = self.__dropout_layer(input_x)\n",
    "        attention_x = self.__attention_layer(\n",
    "            dropout_x, dropout_x, dropout_x\n",
    "        )\n",
    "\n",
    "        flat_x = torch.cat(\n",
    "            [attention_x[i][:seq_lens[i], :] for\n",
    "             i in range(0, len(seq_lens))], dim=0\n",
    "        )\n",
    "        return flat_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Alphabet(object):\n",
    "    \"\"\"\n",
    "    Storage and serialization a set of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, if_use_pad, if_use_unk):\n",
    "\n",
    "        self.__name = name\n",
    "        self.__if_use_pad = if_use_pad\n",
    "        self.__if_use_unk = if_use_unk\n",
    "\n",
    "        self.__index2instance = OrderedSet()\n",
    "        self.__instance2index = OrderedDict()\n",
    "\n",
    "        # Counter Object record the frequency\n",
    "        # of element occurs in raw text.\n",
    "        self.__counter = Counter()\n",
    "\n",
    "        if if_use_pad:\n",
    "            self.__sign_pad = \"<PAD>\"\n",
    "            self.add_instance(self.__sign_pad)\n",
    "        if if_use_unk:\n",
    "            self.__sign_unk = \"<UNK>\"\n",
    "            self.add_instance(self.__sign_unk)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    def add_instance(self, instance):\n",
    "        \"\"\" Add instances to alphabet.\n",
    "        1, We support any iterative data structure which\n",
    "        contains elements of str type.\n",
    "        2, We will count added instances that will influence\n",
    "        the serialization of unknown instance.\n",
    "        :param instance: is given instance or a list of it.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(instance, (list, tuple)):\n",
    "            for element in instance:\n",
    "                self.add_instance(element)\n",
    "            return\n",
    "\n",
    "        # We only support elements of str type.\n",
    "        assert isinstance(instance, str)\n",
    "\n",
    "        # count the frequency of instances.\n",
    "        self.__counter[instance] += 1\n",
    "\n",
    "        if instance not in self.__index2instance:\n",
    "            self.__instance2index[instance] = len(self.__index2instance)\n",
    "            self.__index2instance.append(instance)\n",
    "\n",
    "    def get_index(self, instance):\n",
    "        \"\"\" Serialize given instance and return.\n",
    "        For unknown words, the return index of alphabet\n",
    "        depends on variable self.__use_unk:\n",
    "            1, If True, then return the index of \"<UNK>\";\n",
    "            2, If False, then return the index of the\n",
    "            element that hold max frequency in training data.\n",
    "        :param instance: is given instance or a list of it.\n",
    "        :return: is the serialization of query instance.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(instance, (list, tuple)):\n",
    "            return [self.get_index(elem) for elem in instance]\n",
    "\n",
    "        assert isinstance(instance, str)\n",
    "\n",
    "        try:\n",
    "            return self.__instance2index[instance]\n",
    "        except KeyError:\n",
    "            if self.__if_use_unk:\n",
    "                return self.__instance2index[self.__sign_unk]\n",
    "            else:\n",
    "                max_freq_item = self.__counter.most_common(1)[0][0]\n",
    "                return self.__instance2index[max_freq_item]\n",
    "\n",
    "    def get_instance(self, index):\n",
    "        \"\"\" Get corresponding instance of query index.\n",
    "        if index is invalid, then throws exception.\n",
    "        :param index: is query index, possibly iterable.\n",
    "        :return: is corresponding instance.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(index, list):\n",
    "            return [self.get_instance(elem) for elem in index]\n",
    "\n",
    "        return self.__index2instance[index]\n",
    "\n",
    "    def save_content(self, dir_path):\n",
    "        \"\"\" Save the content of alphabet to files.\n",
    "        There are two kinds of saved files:\n",
    "            1, The first is a list file, elements are\n",
    "            sorted by the frequency of occurrence.\n",
    "            2, The second is a dictionary file, elements\n",
    "            are sorted by it serialized index.\n",
    "        :param dir_path: is the directory path to save object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if dir_path exists.\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "        list_path = os.path.join(dir_path, self.__name + \"_list.txt\")\n",
    "        with open(list_path, 'w') as fw:\n",
    "            for element, frequency in self.__counter.most_common():\n",
    "                fw.write(element + '\\t' + str(frequency) + '\\n')\n",
    "\n",
    "        dict_path = os.path.join(dir_path, self.__name + \"_dict.txt\")\n",
    "        with open(dict_path, 'w') as fw:\n",
    "            for index, element in enumerate(self.__index2instance):\n",
    "                fw.write(element + '\\t' + str(index) + '\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__index2instance)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Alphabet {} contains about {} words: \\n\\t{}'.format(self.name, len(self), self.__index2instance)\n",
    "\n",
    "\n",
    "class TorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Helper class implementing torch.utils.data.Dataset to\n",
    "    instantiate DataLoader which deliveries data batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text, slot, intent):\n",
    "        self.__text = text\n",
    "        self.__slot = slot\n",
    "        self.__intent = intent\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__text[index], self.__slot[index], self.__intent[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Pre-check to avoid bug.\n",
    "        assert len(self.__text) == len(self.__slot)\n",
    "        assert len(self.__text) == len(self.__intent)\n",
    "\n",
    "        return len(self.__text)\n",
    "\n",
    "\n",
    "class DatasetManager(object):\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        # Instantiate alphabet objects.\n",
    "        self.__word_alphabet = Alphabet('word', if_use_pad=True, if_use_unk=True)\n",
    "        self.__slot_alphabet = Alphabet('slot', if_use_pad=False, if_use_unk=False)\n",
    "        self.__intent_alphabet = Alphabet('intent', if_use_pad=False, if_use_unk=False)\n",
    "\n",
    "        # Record the raw text of dataset.\n",
    "        self.__text_word_data = {}\n",
    "        self.__text_slot_data = {}\n",
    "        self.__text_intent_data = {}\n",
    "\n",
    "        # Record the serialization of dataset.\n",
    "        self.__digit_word_data = {}\n",
    "        self.__digit_slot_data = {}\n",
    "        self.__digit_intent_data = {}\n",
    "\n",
    "        self.__args = args\n",
    "\n",
    "    @property\n",
    "    def test_sentence(self):\n",
    "        return deepcopy(self.__text_word_data['test'])\n",
    "\n",
    "    @property\n",
    "    def word_alphabet(self):\n",
    "        return deepcopy(self.__word_alphabet)\n",
    "\n",
    "    @property\n",
    "    def slot_alphabet(self):\n",
    "        return deepcopy(self.__slot_alphabet)\n",
    "\n",
    "    @property\n",
    "    def intent_alphabet(self):\n",
    "        return deepcopy(self.__intent_alphabet)\n",
    "\n",
    "    @property\n",
    "    def num_epoch(self):\n",
    "        return self.__args.num_epoch\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.__args.batch_size\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self.__args.learning_rate\n",
    "\n",
    "    @property\n",
    "    def l2_penalty(self):\n",
    "        return self.__args.l2_penalty\n",
    "\n",
    "    @property\n",
    "    def save_dir(self):\n",
    "        return self.__args.save_dir\n",
    "\n",
    "    @property\n",
    "    def intent_forcing_rate(self):\n",
    "        return self.__args.intent_forcing_rate\n",
    "\n",
    "    @property\n",
    "    def slot_forcing_rate(self):\n",
    "        return self.__args.slot_forcing_rate\n",
    "\n",
    "    def show_summary(self):\n",
    "        \"\"\"\n",
    "        :return: show summary of dataset, training parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Training parameters are listed as follows:\\n\")\n",
    "\n",
    "        print('\\tnumber of train sample:                    {};'.format(len(self.__text_word_data['train'])))\n",
    "        print('\\tnumber of dev sample:                      {};'.format(len(self.__text_word_data['dev'])))\n",
    "        print('\\tnumber of test sample:                     {};'.format(len(self.__text_word_data['test'])))\n",
    "        print('\\tnumber of epoch:\t\t\t\t\t\t    {};'.format(self.num_epoch))\n",
    "        print('\\tbatch size:\t\t\t\t\t\t\t    {};'.format(self.batch_size))\n",
    "        print('\\tlearning rate:\t\t\t\t\t\t\t    {};'.format(self.learning_rate))\n",
    "        print('\\trandom seed:\t\t\t\t\t\t\t    {};'.format(self.__args.random_state))\n",
    "        print('\\trate of l2 penalty:\t\t\t\t\t    {};'.format(self.l2_penalty))\n",
    "        print('\\trate of dropout in network:                {};'.format(self.__args.dropout_rate))\n",
    "        print('\\tteacher forcing rate(slot)\t\t    \t\t{};'.format(self.slot_forcing_rate))\n",
    "        print('\\tteacher forcing rate(intent):\t\t    \t{};'.format(self.intent_forcing_rate))\n",
    "\n",
    "        print(\"\\nEnd of parameters show. Save dir: {}.\\n\\n\".format(self.save_dir))\n",
    "\n",
    "    def quick_build(self):\n",
    "        \"\"\"\n",
    "        Convenient function to instantiate a dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        train_path = os.path.join(self.__args.data_dir, 'train.txt')\n",
    "        dev_path = os.path.join(self.__args.data_dir, 'dev.txt')\n",
    "        test_path = os.path.join(self.__args.data_dir, 'test.txt')\n",
    "\n",
    "        self.add_file(train_path, 'train', if_train_file=True)\n",
    "        self.add_file(dev_path, 'dev', if_train_file=False)\n",
    "        self.add_file(test_path, 'test', if_train_file=False)\n",
    "\n",
    "        # Check if save path exists.\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "\n",
    "        alphabet_dir = os.path.join(self.__args.save_dir, \"alphabet\")\n",
    "        self.__word_alphabet.save_content(alphabet_dir)\n",
    "        self.__slot_alphabet.save_content(alphabet_dir)\n",
    "        self.__intent_alphabet.save_content(alphabet_dir)\n",
    "\n",
    "    def get_dataset(self, data_name, is_digital):\n",
    "        \"\"\" Get dataset of given unique name.\n",
    "        :param data_name: is name of stored dataset.\n",
    "        :param is_digital: make sure if want serialized data.\n",
    "        :return: the required dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if is_digital:\n",
    "            return self.__digit_word_data[data_name], \\\n",
    "                   self.__digit_slot_data[data_name], \\\n",
    "                   self.__digit_intent_data[data_name]\n",
    "        else:\n",
    "            return self.__text_word_data[data_name], \\\n",
    "                   self.__text_slot_data[data_name], \\\n",
    "                   self.__text_intent_data[data_name]\n",
    "\n",
    "    def add_file(self, file_path, data_name, if_train_file):\n",
    "        text, slot, intent = self.__read_file(file_path)\n",
    "\n",
    "        if if_train_file:\n",
    "            self.__word_alphabet.add_instance(text)\n",
    "            self.__slot_alphabet.add_instance(slot)\n",
    "            self.__intent_alphabet.add_instance(intent)\n",
    "\n",
    "        # Record the raw text of dataset.\n",
    "        self.__text_word_data[data_name] = text\n",
    "        self.__text_slot_data[data_name] = slot\n",
    "        self.__text_intent_data[data_name] = intent\n",
    "\n",
    "        # Serialize raw text and stored it.\n",
    "        self.__digit_word_data[data_name] = self.__word_alphabet.get_index(text)\n",
    "        if if_train_file:\n",
    "            self.__digit_slot_data[data_name] = self.__slot_alphabet.get_index(slot)\n",
    "            self.__digit_intent_data[data_name] = self.__intent_alphabet.get_index(intent)\n",
    "\n",
    "    @staticmethod\n",
    "    def __read_file(file_path):\n",
    "        \"\"\" Read data file of given path.\n",
    "        :param file_path: path of data file.\n",
    "        :return: list of sentence, list of slot and list of intent.\n",
    "        \"\"\"\n",
    "\n",
    "        texts, slots, intents = [], [], []\n",
    "        text, slot = [], []\n",
    "\n",
    "        with open(file_path, 'r') as fr:\n",
    "            for line in fr.readlines():\n",
    "                items = line.strip().split()\n",
    "\n",
    "                if len(items) == 1:\n",
    "                    texts.append(text)\n",
    "                    slots.append(slot)\n",
    "                    intents.append(items)\n",
    "\n",
    "                    # clear buffer lists.\n",
    "                    text, slot = [], []\n",
    "\n",
    "                elif len(items) == 2:\n",
    "                    text.append(items[0].strip())\n",
    "                    slot.append(items[1].strip())\n",
    "\n",
    "        return texts, slots, intents\n",
    "\n",
    "    def batch_delivery(self, data_name, batch_size=None, is_digital=True, shuffle=True):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        if is_digital:\n",
    "            text = self.__digit_word_data[data_name]\n",
    "            slot = self.__digit_slot_data[data_name]\n",
    "            intent = self.__digit_intent_data[data_name]\n",
    "        else:\n",
    "            text = self.__text_word_data[data_name]\n",
    "            slot = self.__text_slot_data[data_name]\n",
    "            intent = self.__text_intent_data[data_name]\n",
    "        dataset = TorchDataset(text, slot, intent)\n",
    "\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=self.__collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_padding(texts, items=None, digital=True):\n",
    "        len_list = [len(text) for text in texts]\n",
    "        max_len = max(len_list)\n",
    "\n",
    "        # Get sorted index of len_list.\n",
    "        sorted_index = np.argsort(len_list)[::-1]\n",
    "\n",
    "        trans_texts, seq_lens, trans_items = [], [], None\n",
    "        if items is not None:\n",
    "            trans_items = [[] for _ in range(0, len(items))]\n",
    "\n",
    "        for index in sorted_index:\n",
    "            seq_lens.append(deepcopy(len_list[index]))\n",
    "            trans_texts.append(deepcopy(texts[index]))\n",
    "            if digital:\n",
    "                trans_texts[-1].extend([0] * (max_len - len_list[index]))\n",
    "            else:\n",
    "                trans_texts[-1].extend(['<PAD>'] * (max_len - len_list[index]))\n",
    "\n",
    "            # This required specific if padding after sorting.\n",
    "            if items is not None:\n",
    "                for item, (o_item, required) in zip(trans_items, items):\n",
    "                    item.append(deepcopy(o_item[index]))\n",
    "                    if required:\n",
    "                        if digital:\n",
    "                            item[-1].extend([0] * (max_len - len_list[index]))\n",
    "                        else:\n",
    "                            item[-1].extend(['<PAD>'] * (max_len - len_list[index]))\n",
    "\n",
    "        if items is not None:\n",
    "            return trans_texts, trans_items, seq_lens, sorted_index\n",
    "        else:\n",
    "            return trans_texts, seq_lens, sorted_index\n",
    "\n",
    "    @staticmethod\n",
    "    def __collate_fn(batch):\n",
    "        \"\"\"\n",
    "        helper function to instantiate a DataLoader Object.\n",
    "        \"\"\"\n",
    "\n",
    "        n_entity = len(batch[0])\n",
    "        modified_batch = [[] for _ in range(0, n_entity)]\n",
    "\n",
    "        for idx in range(0, len(batch)):\n",
    "            for jdx in range(0, n_entity):\n",
    "                modified_batch[jdx].append(batch[idx][jdx])\n",
    "\n",
    "        return modified_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Utils functions copied from Slot-gated model, origin url:\n",
    "# \thttps://github.com/MiuLab/SlotGated-SLU/blob/master/utils.py\n",
    "# from utils import miulab\n",
    "\n",
    "\n",
    "class Processor(object):\n",
    "\n",
    "    def __init__(self, dataset, model, batch_size):\n",
    "        self.__dataset = dataset\n",
    "        self.__model = model\n",
    "        self.__batch_size = batch_size\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            time_start = time.time()\n",
    "            self.__model = self.__model.cuda()\n",
    "\n",
    "            time_con = time.time() - time_start\n",
    "            print(\"The model has been loaded into GPU and cost {:.6f} seconds.\\n\".format(time_con))\n",
    "\n",
    "        self.__criterion = nn.NLLLoss()\n",
    "        self.__optimizer = optim.Adam(\n",
    "            self.__model.parameters(), lr=self.__dataset.learning_rate,\n",
    "            weight_decay=self.__dataset.l2_penalty\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        best_dev_slot = 0.0\n",
    "        best_dev_intent = 0.0\n",
    "        best_dev_sent = 0.0\n",
    "\n",
    "        dataloader = self.__dataset.batch_delivery('train')\n",
    "        for epoch in range(0, self.__dataset.num_epoch):\n",
    "            total_slot_loss, total_intent_loss = 0.0, 0.0\n",
    "\n",
    "            time_start = time.time()\n",
    "            self.__model.train()\n",
    "\n",
    "            for text_batch, slot_batch, intent_batch in tqdm(dataloader, ncols=50):\n",
    "                padded_text, [sorted_slot, sorted_intent], seq_lens, _ = self.__dataset.add_padding(\n",
    "                    text_batch, [(slot_batch, False), (intent_batch, False)]\n",
    "                )\n",
    "                sorted_intent = [item * num for item, num in zip(sorted_intent, seq_lens)]\n",
    "                sorted_intent = list(Evaluator.expand_list(sorted_intent))\n",
    "\n",
    "                text_var = Variable(torch.LongTensor(padded_text))\n",
    "                slot_var = Variable(torch.LongTensor(list(Evaluator.expand_list(sorted_slot))))\n",
    "                intent_var = Variable(torch.LongTensor(sorted_intent))\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    text_var = text_var.cuda()\n",
    "                    slot_var = slot_var.cuda()\n",
    "                    intent_var = intent_var.cuda()\n",
    "\n",
    "                random_slot, random_intent = random.random(), random.random()\n",
    "                if random_slot < self.__dataset.slot_forcing_rate and \\\n",
    "                        random_intent < self.__dataset.intent_forcing_rate:\n",
    "                    slot_out, intent_out = self.__model(\n",
    "                        text_var, seq_lens, forced_slot=slot_var, forced_intent=intent_var\n",
    "                    )\n",
    "                elif random_slot < self.__dataset.slot_forcing_rate:\n",
    "                    slot_out, intent_out = self.__model(\n",
    "                        text_var, seq_lens, forced_slot=slot_var\n",
    "                    )\n",
    "                elif random_intent < self.__dataset.intent_forcing_rate:\n",
    "                    slot_out, intent_out = self.__model(\n",
    "                        text_var, seq_lens, forced_intent=intent_var\n",
    "                    )\n",
    "                else:\n",
    "                    slot_out, intent_out = self.__model(text_var, seq_lens)\n",
    "\n",
    "                slot_loss = self.__criterion(slot_out, slot_var)\n",
    "                intent_loss = self.__criterion(intent_out, intent_var)\n",
    "                batch_loss = slot_loss + intent_loss\n",
    "\n",
    "                self.__optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.__optimizer.step()\n",
    "\n",
    "                try:\n",
    "                    total_slot_loss += slot_loss.cpu().item()\n",
    "                    total_intent_loss += intent_loss.cpu().item()\n",
    "                except AttributeError:\n",
    "                    total_slot_loss += slot_loss.cpu().data.numpy()[0]\n",
    "                    total_intent_loss += intent_loss.cpu().data.numpy()[0]\n",
    "\n",
    "            time_con = time.time() - time_start\n",
    "            print('[Epoch {:2d}]: The total slot loss on train data is {:2.6f}, intent data is {:2.6f}, cost ' \\\n",
    "                  'about {:2.6} seconds.'.format(epoch, total_slot_loss, total_intent_loss, time_con))\n",
    "\n",
    "            change, time_start = False, time.time()\n",
    "            dev_f1_score, dev_acc, dev_sent_acc = self.estimate(if_dev=True, test_batch=self.__batch_size)\n",
    "\n",
    "            if dev_f1_score > best_dev_slot or dev_acc > best_dev_intent or dev_sent_acc > best_dev_sent:\n",
    "                test_f1, test_acc, test_sent_acc = self.estimate(if_dev=False, test_batch=self.__batch_size)\n",
    "\n",
    "                if dev_f1_score > best_dev_slot:\n",
    "                    best_dev_slot = dev_f1_score\n",
    "                if dev_acc > best_dev_intent:\n",
    "                    best_dev_intent = dev_acc\n",
    "                if dev_sent_acc > best_dev_sent:\n",
    "                    best_dev_sent = dev_sent_acc\n",
    "\n",
    "                print('\\nTest result: slot f1 score: {:.6f}, intent acc score: {:.6f}, semantic '\n",
    "                      'accuracy score: {:.6f}.'.format(test_f1, test_acc, test_sent_acc))\n",
    "\n",
    "                model_save_dir = os.path.join(self.__dataset.save_dir, \"model\")\n",
    "                if not os.path.exists(model_save_dir):\n",
    "                    os.mkdir(model_save_dir)\n",
    "\n",
    "                torch.save(self.__model, os.path.join(model_save_dir, \"model.pkl\"))\n",
    "                torch.save(self.__dataset, os.path.join(model_save_dir, 'dataset.pkl'))\n",
    "\n",
    "                time_con = time.time() - time_start\n",
    "                print('[Epoch {:2d}]: In validation process, the slot f1 score is {:2.6f}, ' \\\n",
    "                      'the intent acc is {:2.6f}, the semantic acc is {:.2f}, cost about ' \\\n",
    "                      '{:2.6f} seconds.\\n'.format(epoch, dev_f1_score, dev_acc, dev_sent_acc, time_con))\n",
    "\n",
    "    def estimate(self, if_dev, test_batch=100):\n",
    "        \"\"\"\n",
    "        Estimate the performance of model on dev or test dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if if_dev:\n",
    "            pred_slot, real_slot, pred_intent, real_intent, _ = self.prediction(\n",
    "                self.__model, self.__dataset, \"dev\", test_batch\n",
    "            )\n",
    "        else:\n",
    "            pred_slot, real_slot, pred_intent, real_intent, _ = self.prediction(\n",
    "                self.__model, self.__dataset, \"test\", test_batch\n",
    "            )\n",
    "\n",
    "        slot_f1_socre = computeF1Score(pred_slot, real_slot)[0]\n",
    "        intent_acc = Evaluator.accuracy(pred_intent, real_intent)\n",
    "        sent_acc = Evaluator.semantic_acc(pred_slot, real_slot, pred_intent, real_intent)\n",
    "\n",
    "        return slot_f1_socre, intent_acc, sent_acc\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(model_path, dataset_path, batch_size):\n",
    "        \"\"\"\n",
    "        validation will write mistaken samples to files and make scores.\n",
    "        \"\"\"\n",
    "\n",
    "        model = torch.load(model_path)\n",
    "        dataset = torch.load(dataset_path)\n",
    "\n",
    "        # Get the sentence list in test dataset.\n",
    "        sent_list = dataset.test_sentence\n",
    "\n",
    "        pred_slot, real_slot, exp_pred_intent, real_intent, pred_intent = Processor.prediction(\n",
    "            model, dataset, \"test\", batch_size\n",
    "        )\n",
    "\n",
    "        # To make sure the directory for save error prediction.\n",
    "        mistake_dir = os.path.join(dataset.save_dir, \"error\")\n",
    "        if not os.path.exists(mistake_dir):\n",
    "            os.mkdir(mistake_dir)\n",
    "\n",
    "        slot_file_path = os.path.join(mistake_dir, \"slot.txt\")\n",
    "        intent_file_path = os.path.join(mistake_dir, \"intent.txt\")\n",
    "        both_file_path = os.path.join(mistake_dir, \"both.txt\")\n",
    "\n",
    "        # Write those sample with mistaken slot prediction.\n",
    "        with open(slot_file_path, 'w') as fw:\n",
    "            for w_list, r_slot_list, p_slot_list in zip(sent_list, real_slot, pred_slot):\n",
    "                if r_slot_list != p_slot_list:\n",
    "                    for w, r, p in zip(w_list, r_slot_list, p_slot_list):\n",
    "                        fw.write(w + '\\t' + r + '\\t' + p + '\\n')\n",
    "                    fw.write('\\n')\n",
    "\n",
    "        # Write those sample with mistaken intent prediction.\n",
    "        with open(intent_file_path, 'w') as fw:\n",
    "            for w_list, p_intent_list, r_intent, p_intent in zip(sent_list, pred_intent, real_intent, exp_pred_intent):\n",
    "                if p_intent != r_intent:\n",
    "                    for w, p in zip(w_list, p_intent_list):\n",
    "                        fw.write(w + '\\t' + p + '\\n')\n",
    "                    fw.write(r_intent + '\\t' + p_intent + '\\n\\n')\n",
    "\n",
    "        # Write those sample both have intent and slot errors.\n",
    "        with open(both_file_path, 'w') as fw:\n",
    "            for w_list, r_slot_list, p_slot_list, p_intent_list, r_intent, p_intent in \\\n",
    "                    zip(sent_list, real_slot, pred_slot, pred_intent, real_intent, exp_pred_intent):\n",
    "\n",
    "                if r_slot_list != p_slot_list or r_intent != p_intent:\n",
    "                    for w, r_slot, p_slot, p_intent_ in zip(w_list, r_slot_list, p_slot_list, p_intent_list):\n",
    "                        fw.write(w + '\\t' + r_slot + '\\t' + p_slot + '\\t' + p_intent_ + '\\n')\n",
    "                    fw.write(r_intent + '\\t' + p_intent + '\\n\\n')\n",
    "\n",
    "        slot_f1 = computeF1Score(pred_slot, real_slot)[0]\n",
    "        intent_acc = Evaluator.accuracy(exp_pred_intent, real_intent)\n",
    "        sent_acc = Evaluator.semantic_acc(pred_slot, real_slot, exp_pred_intent, real_intent)\n",
    "\n",
    "        return slot_f1, intent_acc, sent_acc\n",
    "\n",
    "    @staticmethod\n",
    "    def prediction(model, dataset, mode, batch_size):\n",
    "        model.eval()\n",
    "\n",
    "        if mode == \"dev\":\n",
    "            dataloader = dataset.batch_delivery('dev', batch_size=batch_size, shuffle=False, is_digital=False)\n",
    "        elif mode == \"test\":\n",
    "            dataloader = dataset.batch_delivery('test', batch_size=batch_size, shuffle=False, is_digital=False)\n",
    "        else:\n",
    "            raise Exception(\"Argument error! mode belongs to {\\\"dev\\\", \\\"test\\\"}.\")\n",
    "\n",
    "        pred_slot, real_slot = [], []\n",
    "        pred_intent, real_intent = [], []\n",
    "\n",
    "        for text_batch, slot_batch, intent_batch in tqdm(dataloader, ncols=50):\n",
    "            padded_text, [sorted_slot, sorted_intent], seq_lens, sorted_index = dataset.add_padding(\n",
    "                text_batch, [(slot_batch, False), (intent_batch, False)], digital=False\n",
    "            )\n",
    "            # Because it's a visualization bug, in valid time, it doesn't matter\n",
    "            # Only in test time will it need to restore\n",
    "            if mode == 'test':\n",
    "                tmp_r_slot = [[] for _ in range(len(sorted_index))]\n",
    "                for i in range(len(sorted_index)):\n",
    "                    tmp_r_slot[sorted_index[i]] = sorted_slot[i]\n",
    "                sorted_slot = tmp_r_slot\n",
    "                tmp_intent = [[] for _ in range(len(sorted_index))]\n",
    "                for i in range(len(sorted_index)):\n",
    "                    tmp_intent[sorted_index[i]] = sorted_intent[i]\n",
    "                sorted_intent = tmp_intent\n",
    "            \n",
    "            real_slot.extend(sorted_slot)\n",
    "            real_intent.extend(list(Evaluator.expand_list(sorted_intent)))\n",
    "\n",
    "            digit_text = dataset.word_alphabet.get_index(padded_text)\n",
    "            var_text = Variable(torch.LongTensor(digit_text))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                var_text = var_text.cuda()\n",
    "\n",
    "            slot_idx, intent_idx = model(var_text, seq_lens, n_predicts=1)\n",
    "            nested_slot = Evaluator.nested_list([list(Evaluator.expand_list(slot_idx))], seq_lens)[0]\n",
    "            \n",
    "            if mode == 'test':\n",
    "                tmp_r_slot = [[] for _ in range(len(sorted_index))]\n",
    "                for i in range(len(sorted_index)):\n",
    "                    tmp_r_slot[sorted_index[i]] = nested_slot[i]\n",
    "                nested_slot = tmp_r_slot\n",
    "            \n",
    "            pred_slot.extend(dataset.slot_alphabet.get_instance(nested_slot))\n",
    "            nested_intent = Evaluator.nested_list([list(Evaluator.expand_list(intent_idx))], seq_lens)[0]\n",
    "            \n",
    "            if mode == 'test':\n",
    "                tmp_intent = [[] for _ in range(len(sorted_index))]\n",
    "                for i in range(len(sorted_index)):\n",
    "                    tmp_intent[sorted_index[i]] = nested_intent[i]\n",
    "                nested_intent = tmp_intent\n",
    "            \n",
    "            pred_intent.extend(dataset.intent_alphabet.get_instance(nested_intent))\n",
    "\n",
    "        exp_pred_intent = Evaluator.max_freq_predict(pred_intent)\n",
    "        return pred_slot, real_slot, exp_pred_intent, real_intent, pred_intent\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def semantic_acc(pred_slot, real_slot, pred_intent, real_intent):\n",
    "        \"\"\"\n",
    "        Compute the accuracy based on the whole predictions of\n",
    "        given sentence, including slot and intent.\n",
    "        \"\"\"\n",
    "\n",
    "        total_count, correct_count = 0.0, 0.0\n",
    "        for p_slot, r_slot, p_intent, r_intent in zip(pred_slot, real_slot, pred_intent, real_intent):\n",
    "\n",
    "            if p_slot == r_slot and p_intent == r_intent:\n",
    "                correct_count += 1.0\n",
    "            total_count += 1.0\n",
    "\n",
    "        return 1.0 * correct_count / total_count\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(pred_list, real_list):\n",
    "        \"\"\"\n",
    "        Get accuracy measured by predictions and ground-trues.\n",
    "        \"\"\"\n",
    "\n",
    "        pred_array = np.array(list(Evaluator.expand_list(pred_list)))\n",
    "        real_array = np.array(list(Evaluator.expand_list(real_list)))\n",
    "        return (pred_array == real_array).sum() * 1.0 / len(pred_array)\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(pred_list, real_list):\n",
    "        \"\"\"\n",
    "        Get F1 score measured by predictions and ground-trues.\n",
    "        \"\"\"\n",
    "\n",
    "        tp, fp, fn = 0.0, 0.0, 0.0\n",
    "        for i in range(len(pred_list)):\n",
    "            seg = set()\n",
    "            result = [elem.strip() for elem in pred_list[i]]\n",
    "            target = [elem.strip() for elem in real_list[i]]\n",
    "\n",
    "            j = 0\n",
    "            while j < len(target):\n",
    "                cur = target[j]\n",
    "                if cur[0] == 'B':\n",
    "                    k = j + 1\n",
    "                    while k < len(target):\n",
    "                        str_ = target[k]\n",
    "                        if not (str_[0] == 'I' and cur[1:] == str_[1:]):\n",
    "                            break\n",
    "                        k = k + 1\n",
    "                    seg.add((cur, j, k - 1))\n",
    "                    j = k - 1\n",
    "                j = j + 1\n",
    "\n",
    "            tp_ = 0\n",
    "            j = 0\n",
    "            while j < len(result):\n",
    "                cur = result[j]\n",
    "                if cur[0] == 'B':\n",
    "                    k = j + 1\n",
    "                    while k < len(result):\n",
    "                        str_ = result[k]\n",
    "                        if not (str_[0] == 'I' and cur[1:] == str_[1:]):\n",
    "                            break\n",
    "                        k = k + 1\n",
    "                    if (cur, j, k - 1) in seg:\n",
    "                        tp_ += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "                    j = k - 1\n",
    "                j = j + 1\n",
    "\n",
    "            fn += len(seg) - tp_\n",
    "            tp += tp_\n",
    "\n",
    "        p = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "        r = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "        return 2 * p * r / (p + r) if p + r != 0 else 0\n",
    "\n",
    "    \"\"\"\n",
    "    Max frequency prediction. \n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def max_freq_predict(sample):\n",
    "        predict = []\n",
    "        for items in sample:\n",
    "            predict.append(Counter(items).most_common(1)[0][0])\n",
    "        return predict\n",
    "\n",
    "    @staticmethod\n",
    "    def exp_decay_predict(sample, decay_rate=0.8):\n",
    "        predict = []\n",
    "        for items in sample:\n",
    "            item_dict = {}\n",
    "            curr_weight = 1.0\n",
    "            for item in items[::-1]:\n",
    "                item_dict[item] = item_dict.get(item, 0) + curr_weight\n",
    "                curr_weight *= decay_rate\n",
    "            predict.append(sorted(item_dict.items(), key=lambda x_: x_[1])[-1][0])\n",
    "        return predict\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_list(nested_list):\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, (list, tuple)):\n",
    "                for sub_item in Evaluator.expand_list(item):\n",
    "                    yield sub_item\n",
    "            else:\n",
    "                yield item\n",
    "\n",
    "    @staticmethod\n",
    "    def nested_list(items, seq_lens):\n",
    "        num_items = len(items)\n",
    "        trans_items = [[] for _ in range(0, num_items)]\n",
    "\n",
    "        count = 0\n",
    "        for jdx in range(0, len(seq_lens)):\n",
    "            for idx in range(0, num_items):\n",
    "                trans_items[idx].append(items[idx][count:count + seq_lens[jdx]])\n",
    "            count += seq_lens[jdx]\n",
    "\n",
    "        return trans_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miu Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute f1 score is modified from conlleval.pl\n",
    "def __startOfChunk(prevTag, tag, prevTagType, tagType, chunkStart=False):\n",
    "\tif prevTag == 'B' and tag == 'B':\n",
    "\t\tchunkStart = True\n",
    "\tif prevTag == 'I' and tag == 'B':\n",
    "\t\tchunkStart = True\n",
    "\tif prevTag == 'O' and tag == 'B':\n",
    "\t\tchunkStart = True\n",
    "\tif prevTag == 'O' and tag == 'I':\n",
    "\t\tchunkStart = True\n",
    "\n",
    "\tif prevTag == 'E' and tag == 'E':\n",
    "\t\tchunkStart = True\n",
    "\tif prevTag == 'E' and tag == 'I':\n",
    "\t\tchunkStart = True\n",
    "\tif prevTag == 'O' and tag == 'E':\n",
    "\t\tchunkStart = True\n",
    "\tif prevTag == 'O' and tag == 'I':\n",
    "\t\tchunkStart = True\n",
    "\n",
    "\tif tag != 'O' and tag != '.' and prevTagType != tagType:\n",
    "\t\tchunkStart = True\n",
    "\treturn chunkStart\n",
    "\n",
    "\n",
    "def __endOfChunk(prevTag, tag, prevTagType, tagType, chunkEnd=False):\n",
    "\tif prevTag == 'B' and tag == 'B':\n",
    "\t\tchunkEnd = True\n",
    "\tif prevTag == 'B' and tag == 'O':\n",
    "\t\tchunkEnd = True\n",
    "\tif prevTag == 'I' and tag == 'B':\n",
    "\t\tchunkEnd = True\n",
    "\tif prevTag == 'I' and tag == 'O':\n",
    "\t\tchunkEnd = True\n",
    "\n",
    "\tif prevTag == 'E' and tag == 'E':\n",
    "\t\tchunkEnd = True\n",
    "\tif prevTag == 'E' and tag == 'I':\n",
    "\t\tchunkEnd = True\n",
    "\tif prevTag == 'E' and tag == 'O':\n",
    "\t\tchunkEnd = True\n",
    "\tif prevTag == 'I' and tag == 'O':\n",
    "\t\tchunkEnd = True\n",
    "\n",
    "\tif prevTag != 'O' and prevTag != '.' and prevTagType != tagType:\n",
    "\t\tchunkEnd = True\n",
    "\treturn chunkEnd\n",
    "\n",
    "\n",
    "def __splitTagType(tag):\n",
    "\ts = tag.split('-')\n",
    "\tif len(s) > 2 or len(s) == 0:\n",
    "\t\traise ValueError('tag format wrong. it must be B-xxx.xxx')\n",
    "\tif len(s) == 1:\n",
    "\t\ttag = s[0]\n",
    "\t\ttagType = \"\"\n",
    "\telse:\n",
    "\t\ttag = s[0]\n",
    "\t\ttagType = s[1]\n",
    "\treturn tag, tagType\n",
    "\n",
    "\n",
    "def computeF1Score(correct_slots, pred_slots):\n",
    "\tcorrectChunk = {}\n",
    "\tcorrectChunkCnt = 0.0\n",
    "\tfoundCorrect = {}\n",
    "\tfoundCorrectCnt = 0.0\n",
    "\tfoundPred = {}\n",
    "\tfoundPredCnt = 0.0\n",
    "\tcorrectTags = 0.0\n",
    "\ttokenCount = 0.0\n",
    "\tfor correct_slot, pred_slot in zip(correct_slots, pred_slots):\n",
    "\t\tinCorrect = False\n",
    "\t\tlastCorrectTag = 'O'\n",
    "\t\tlastCorrectType = ''\n",
    "\t\tlastPredTag = 'O'\n",
    "\t\tlastPredType = ''\n",
    "\t\tfor c, p in zip(correct_slot, pred_slot):\n",
    "\t\t\tcorrectTag, correctType = __splitTagType(c)\n",
    "\t\t\tpredTag, predType = __splitTagType(p)\n",
    "\n",
    "\t\t\tif inCorrect == True:\n",
    "\t\t\t\tif __endOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True and \\\n",
    "\t\t\t\t\t__endOfChunk(lastPredTag, predTag, lastPredType, predType) == True and \\\n",
    "\t\t\t\t\t(lastCorrectType == lastPredType):\n",
    "\t\t\t\t\tinCorrect = False\n",
    "\t\t\t\t\tcorrectChunkCnt += 1.0\n",
    "\t\t\t\t\tif lastCorrectType in correctChunk:\n",
    "\t\t\t\t\t\tcorrectChunk[lastCorrectType] += 1.0\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcorrectChunk[lastCorrectType] = 1.0\n",
    "\t\t\t\telif __endOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) != \\\n",
    "\t\t\t\t\t__endOfChunk(lastPredTag, predTag, lastPredType, predType) or \\\n",
    "\t\t\t\t\t(correctType != predType):\n",
    "\t\t\t\t\tinCorrect = False\n",
    "\n",
    "\t\t\tif __startOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True and \\\n",
    "\t\t\t\t__startOfChunk(lastPredTag, predTag, lastPredType, predType) == True and \\\n",
    "\t\t\t\t(correctType == predType):\n",
    "\t\t\t\tinCorrect = True\n",
    "\n",
    "\t\t\tif __startOfChunk(lastCorrectTag, correctTag, lastCorrectType, correctType) == True:\n",
    "\t\t\t\tfoundCorrectCnt += 1\n",
    "\t\t\t\tif correctType in foundCorrect:\n",
    "\t\t\t\t\tfoundCorrect[correctType] += 1.0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfoundCorrect[correctType] = 1.0\n",
    "\n",
    "\t\t\tif __startOfChunk(lastPredTag, predTag, lastPredType, predType) == True:\n",
    "\t\t\t\tfoundPredCnt += 1.0\n",
    "\t\t\t\tif predType in foundPred:\n",
    "\t\t\t\t\tfoundPred[predType] += 1.0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tfoundPred[predType] = 1.0\n",
    "\n",
    "\t\t\tif correctTag == predTag and correctType == predType:\n",
    "\t\t\t\tcorrectTags += 1.0\n",
    "\n",
    "\t\t\ttokenCount += 1.0\n",
    "\n",
    "\t\t\tlastCorrectTag = correctTag\n",
    "\t\t\tlastCorrectType = correctType\n",
    "\t\t\tlastPredTag = predTag\n",
    "\t\t\tlastPredType = predType\n",
    "\n",
    "\t\tif inCorrect == True:\n",
    "\t\t\tcorrectChunkCnt += 1.0\n",
    "\t\t\tif lastCorrectType in correctChunk:\n",
    "\t\t\t\tcorrectChunk[lastCorrectType] += 1.0\n",
    "\t\t\telse:\n",
    "\t\t\t\tcorrectChunk[lastCorrectType] = 1.0\n",
    "\n",
    "\tif foundPredCnt > 0:\n",
    "\t\tprecision = 1.0 * correctChunkCnt / foundPredCnt\n",
    "\telse:\n",
    "\t\tprecision = 0\n",
    "\n",
    "\tif foundCorrectCnt > 0:\n",
    "\t\trecall = 1.0 * correctChunkCnt / foundCorrectCnt\n",
    "\telse:\n",
    "\t\trecall = 0\n",
    "\n",
    "\tif (precision + recall) > 0:\n",
    "\t\tf1 = (2.0 * precision * recall) / (precision + recall)\n",
    "\telse:\n",
    "\t\tf1 = 0\n",
    "\n",
    "\treturn f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-31 15:06:06 nemo_logging:349] /Users/sorenmadsen/opt/anaconda3/envs/nemo/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "      warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters are listed as follows:\n",
      "\n",
      "\tnumber of train sample:                    13093;\n",
      "\tnumber of dev sample:                      700;\n",
      "\tnumber of test sample:                     700;\n",
      "\tnumber of epoch:\t\t\t\t\t\t    60;\n",
      "\tbatch size:\t\t\t\t\t\t\t    32;\n",
      "\tlearning rate:\t\t\t\t\t\t\t    0.001;\n",
      "\trandom seed:\t\t\t\t\t\t\t    42;\n",
      "\trate of l2 penalty:\t\t\t\t\t    1e-06;\n",
      "\trate of dropout in network:                0.4;\n",
      "\tteacher forcing rate(slot)\t\t    \t\t0.9;\n",
      "\tteacher forcing rate(intent):\t\t    \t0.9;\n",
      "\n",
      "End of parameters show. Save dir: ./save/.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "# Training Params\n",
    "args.save_dir = './save/'\n",
    "args.random_state = 42\n",
    "args.batch_size = 32\n",
    "args.num_epoch = 60\n",
    "args.l2_penalty = 1e-6\n",
    "args.dropout_rate = 0.4\n",
    "args.learning_rate = 1e-3\n",
    "args.intent_forcing_rate = 0.9\n",
    "args.slot_forcing_rate = 0.9\n",
    "args.differentiable= False\n",
    "args.data_dir = './data/data/'\n",
    "\n",
    "# Model params\n",
    "args.word_embedding_dim = 64\n",
    "args.encoder_hidden_dim= 256\n",
    "args.intent_embedding_dim = 8\n",
    "args.slot_embedding_dim = 32\n",
    "args.slot_decoder_hidden_dim = 64\n",
    "args.intent_decoder_hidden_dim = 64\n",
    "args.attention_hidden_dim = 1024\n",
    "args.attention_output_dim = 128\n",
    "\n",
    "\n",
    "# Save training and model parameters.\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.system(\"mkdir -p \" + args.save_dir)\n",
    "\n",
    "log_path = os.path.join(args.save_dir, \"param.json\")\n",
    "\n",
    "# Fix the random seed of package random.\n",
    "random.seed(args.random_state)\n",
    "np.random.seed(args.random_state)\n",
    "\n",
    "# Fix the random seed of Pytorch when using GPU.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.random_state)\n",
    "    torch.cuda.manual_seed(args.random_state)\n",
    "\n",
    "# Fix the random seed of Pytorch when using CPU.\n",
    "torch.manual_seed(args.random_state)\n",
    "torch.random.manual_seed(args.random_state)\n",
    "\n",
    "# Instantiate a dataset object.\n",
    "dataset = DatasetManager(args)\n",
    "dataset.quick_build()\n",
    "dataset.show_summary()\n",
    "\n",
    "# # Instantiate a network model object.\n",
    "model = ModelManager(\n",
    "    args, len(dataset.word_alphabet),\n",
    "    len(dataset.slot_alphabet),\n",
    "    len(dataset.intent_alphabet))\n",
    "# model.show_summary()\n",
    "\n",
    "# # To train and evaluate the models.\n",
    "# process = Processor(dataset, model, args.batch_size)\n",
    "# process.train()\n",
    "\n",
    "# print('\\nAccepted performance: ' + str(Processor.validate(\n",
    "#     os.path.join(args.save_dir, \"model/model.pkl\"),\n",
    "#     os.path.join(args.save_dir, \"model/dataset.pkl\"),\n",
    "\n",
    "#     args.batch_size)) + \" at test dataset;\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_bLbXUbRyqT"
   },
   "source": [
    "# Streaming Inference\n",
    "\n",
    "Streaming inference depends on a few factors, such as the frame length and buffer size. Experiment with a few values to see their effects in the below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webrtcvad in /Users/sorenmadsen/opt/anaconda3/envs/nemo/lib/python3.9/site-packages (2.0.10)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import struct\n",
    "asr_model.eval()\n",
    "class VADTranscriber():\n",
    "    def __init__(self, aggressiveness, sample_rate=16000):\n",
    "        self.vad = webrtcvad.Vad(aggressiveness)\n",
    "        self.channels=1\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_format = pyaudio.paInt16      # 16 bits per sample\n",
    "        self.chunk = (int)(sample_rate * 0.030) # 50ms of data\n",
    "        self.p = pyaudio.PyAudio()\n",
    "    \n",
    "    def _is_speech(self, frame):\n",
    "        return self.vad.is_speech(frame, self.sample_rate)\n",
    "    \n",
    "    def greedy_merge(self, s):\n",
    "        s_merged = ''\n",
    "        prev_char = '_'\n",
    "        for i in range(len(s)):\n",
    "            if s[i] != prev_char:\n",
    "                prev_char = s[i]\n",
    "                if prev_char != '_':\n",
    "                    s_merged += prev_char\n",
    "        return s_merged\n",
    "    \n",
    "    def _greedy_decoder(self,logits, vocab):\n",
    "        s = ''\n",
    "        for i in range(logits.shape[0]):\n",
    "            for j in range(logits.shape[1]):\n",
    "                imax = np.argmax(logits[i][j])\n",
    "                s += vocab[imax]\n",
    "        return s\n",
    "    \n",
    "    def listen(self):\n",
    "        stream = self.p.open(format=self.sample_format,\n",
    "                channels = self.channels,\n",
    "                rate = self.sample_rate,\n",
    "                frames_per_buffer = self.chunk,\n",
    "                input = True)\n",
    "        print(\"Now listening\")\n",
    "        voice = False\n",
    "        loop = True\n",
    "        buffer = []\n",
    "        last_voice = 0\n",
    "        voice_thresh = 10\n",
    "        voice_count = voice_thresh\n",
    "        total_frames = 0\n",
    "        while loop:\n",
    "            data = stream.read(self.chunk)\n",
    "                            \n",
    "            if voice:   \n",
    "                buffer.append(data)\n",
    "                total_frames = total_frames + 1\n",
    "                    \n",
    "            if self._is_speech(data):\n",
    "                voice_count = voice_count - 1\n",
    "                if not voice:\n",
    "                    buffer.append(data)\n",
    "                if voice_count == 0:\n",
    "                    voice = True\n",
    "                    print(\"Detected voice\", last_voice)\n",
    "                last_voice = total_frames\n",
    "                \n",
    "            elif not voice: \n",
    "                if voice_count < voice_thresh:\n",
    "                    voice_count = voice_count + 1\n",
    "                else:\n",
    "                    buffer = []\n",
    "            \n",
    "            if voice and (total_frames - last_voice > 30):\n",
    "                print(\"Stopped detecting voice\")\n",
    "                voice = False\n",
    "                loop = False\n",
    "        \n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        self.p.terminate()\n",
    "        buffer = b''.join(buffer)\n",
    "        buffer_len = len(buffer)\n",
    "        data_buf = np.frombuffer(buffer, dtype=np.int16).astype(np.float32)/32768\n",
    "        buffer = torch.as_tensor(data_buf, dtype=torch.float32).to(asr_model.device)\n",
    "        buffer = buffer[None, :]\n",
    "        buffer_len = torch.as_tensor(buffer.size(), dtype=torch.int64).to(asr_model.device)\n",
    "        log_probs, encoded_len, predictions = asr_model.forward(\n",
    "            input_signal=buffer, \n",
    "            input_signal_length=buffer_len)\n",
    "        vocab = list(cfg.decoder.vocabulary)\n",
    "        decoded = self._greedy_decoder(log_probs, vocab)\n",
    "        merged = self.greedy_merge(decoded)\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet intent contains about 8 words: \n",
      "\tOrderedSet(['PlayMusic', 'AddToPlaylist', 'RateBook', 'SearchScreeningEvent', 'BookRestaurant', 'GetWeather', 'SearchCreativeWork', 'HELP'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.load('./HelpModel/model.pkl', map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "\n",
    "words_alphabet = dataset.word_alphabet\n",
    "slots_alphabet = dataset.slot_alphabet\n",
    "intent_alphabet = dataset.intent_alphabet\n",
    "print(intent_alphabet)\n",
    "\n",
    "def nlu(words):\n",
    "    var_text = Variable(torch.LongTensor([words_alphabet.get_index(words)]))\n",
    "    seq_lens = Variable(torch.LongTensor([len(words)]))\n",
    "    if torch.cuda.is_available():\n",
    "        var_text = var_text.cuda()\n",
    "    slot_idx, intent_idx = model(var_text, seq_lens, n_predicts=1)\n",
    "    nested_slots = Evaluator.nested_list([list(Evaluator.expand_list(slot_idx))], seq_lens)[0]\n",
    "    slots = slots_alphabet.get_instance(nested_slots)\n",
    "    nested_intent = Evaluator.nested_list([list(Evaluator.expand_list(intent_idx))], seq_lens)[0]\n",
    "    intent = Evaluator.max_freq_predict(nested_intent)\n",
    "    for i in range(0, len(words)):\n",
    "        print(words[i], \": \\t\", slots[0][i])\n",
    "\n",
    "    print(intent_alphabet.get_instance(intent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now listening\n",
      "Detected voice 0\n",
      "Stopped detecting voice\n",
      "what's : \t O\n",
      "the : \t O\n",
      "weather : \t O\n",
      "in : \t O\n",
      "san : \t B-city\n",
      "francisco : \t I-city\n",
      "['GetWeather']\n"
     ]
    }
   ],
   "source": [
    "transcriber = VADTranscriber(0)\n",
    "text = transcriber.listen().split(' ')\n",
    "if len(text) < 3:\n",
    "    print(\"Command not recognized\")\n",
    "else:\n",
    "    nlu(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Now Recording-----\n",
      "-----Finished Recording-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033c15cdcb4c4e5cb968a81d491a1867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-23 12:50:09 nemo_logging:349] /Users/sorenmadsen/opt/anaconda3/envs/nemo/lib/python3.9/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "      warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio in ./soundsample.wav was recognized as: hello\n",
      "[[6]]\n",
      "hello : \t B-object_name\n",
      "['SearchCreativeWork']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyaudio\n",
    "model = torch.load('./data/model_60epochs.pkl')\n",
    "model.eval()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "words_alphabet = dataset.word_alphabet\n",
    "slots_alphabet = dataset.slot_alphabet\n",
    "intent_alphabet = dataset.intent_alphabet\n",
    "\n",
    "def nlu(words):\n",
    "    var_text = Variable(torch.LongTensor([words_alphabet.get_index(words)]))\n",
    "    seq_lens = Variable(torch.LongTensor([len(words)]))\n",
    "    if torch.cuda.is_available():\n",
    "        var_text = var_text.cuda()\n",
    "    slot_idx, intent_idx = model(var_text, seq_lens, n_predicts=1)\n",
    "    print(intent_idx)\n",
    "    plt.plot(intent_idx)\n",
    "    nested_slots = Evaluator.nested_list([list(Evaluator.expand_list(slot_idx))], seq_lens)[0]\n",
    "    slots = slots_alphabet.get_instance(nested_slots)\n",
    "    nested_intent = Evaluator.nested_list([list(Evaluator.expand_list(intent_idx))], seq_lens)[0]\n",
    "    intent = Evaluator.max_freq_predict(nested_intent)\n",
    "    for i in range(0, len(words)):\n",
    "        print(words[i], \": \\t\", slots[0][i])\n",
    "\n",
    "    print(intent_alphabet.get_instance(intent))\n",
    "\n",
    "import wave\n",
    " \n",
    "chunk = 1024      # Each chunk will consist of 1024 samples\n",
    "sample_format = pyaudio.paInt16      # 16 bits per sample\n",
    "channels = 1      # Number of audio channels\n",
    "fs = 16000        # Record at 16000 samples per second\n",
    "time_in_seconds = 3\n",
    "filename = \"./soundsample.wav\"\n",
    " \n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    " \n",
    "print('-----Now Recording-----')\n",
    " \n",
    "#Open a Stream with the values we just defined\n",
    "stream = p.open(format=sample_format,\n",
    "                channels = channels,\n",
    "                rate = fs,\n",
    "                frames_per_buffer = chunk,\n",
    "                input = True)\n",
    " \n",
    "frames = []  # Initialize array to store frames\n",
    " \n",
    "# Store data in chunks for 3 seconds\n",
    "for i in range(0, int(fs / chunk * time_in_seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    " \n",
    "# Stop and close the Stream and PyAudio\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    " \n",
    "print('-----Finished Recording-----')\n",
    " \n",
    "# Open and Set the data of the WAV file\n",
    "file = wave.open(filename, 'wb')\n",
    "file.setnchannels(channels)\n",
    "file.setsampwidth(p.get_sample_size(sample_format))\n",
    "file.setframerate(fs)\n",
    " \n",
    "#Write and Close the File\n",
    "file.writeframes(b''.join(frames))\n",
    "file.close()\n",
    "\n",
    "files = ['./soundsample.wav']\n",
    "for fname, transcription in zip(files, asr_model.transcribe(paths2audio_files=files,num_workers=0)):\n",
    "    print(f\"Audio in {fname} was recognized as: {transcription}\")\n",
    "    nlu(transcription.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Wwhd1cAVRyqT"
   },
   "outputs": [],
   "source": [
    "# duration of signal frame, seconds\n",
    "FRAME_LEN = 1.0\n",
    "# number of audio channels (expect mono signal)\n",
    "CHANNELS = 1\n",
    "\n",
    "CHUNK_SIZE = int(FRAME_LEN*SAMPLE_RATE)\n",
    "asr = FrameASR(model_definition = {\n",
    "                   'sample_rate': SAMPLE_RATE,\n",
    "                   'AudioToMelSpectrogramPreprocessor': cfg.preprocessor,\n",
    "                   'JasperEncoder': cfg.encoder,\n",
    "                   'labels': cfg.decoder.vocabulary\n",
    "               },\n",
    "               frame_len=FRAME_LEN, frame_overlap=2, \n",
    "               offset=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHX__z7_RyqT",
    "outputId": "15400b70-40a5-494f-b7e1-854495c66cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio input devices:\n",
      "0 Built-in Microphone\n",
      "Please type input device ID:\n",
      "0\n",
      "Listening...\n",
      "hellooh\n",
      "PyAudio stopped\n",
      "hellooh\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mis_active():\n\u001b[0;32m---> 52\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:        \n\u001b[1;32m     54\u001b[0m     stream\u001b[38;5;241m.\u001b[39mstop_stream()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "asr.reset()\n",
    "\n",
    "p = pa.PyAudio()\n",
    "print('Available audio input devices:')\n",
    "input_devices = []\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    if dev.get('maxInputChannels'):\n",
    "        input_devices.append(i)\n",
    "        print(i, dev.get('name'))\n",
    "\n",
    "if len(input_devices):\n",
    "    dev_idx = -2\n",
    "    while dev_idx not in input_devices:\n",
    "        print('Please type input device ID:')\n",
    "        dev_idx = int(input())\n",
    "\n",
    "    empty_counter = 0\n",
    "    transcription = \"\"\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        global empty_counter\n",
    "        global transcription\n",
    "        signal = np.frombuffer(in_data, dtype=np.int16)\n",
    "        text = asr.transcribe(signal)\n",
    "        if len(text):\n",
    "            print(text,end='')\n",
    "            transcription = transcription + text\n",
    "            empty_counter = asr.offset\n",
    "        elif empty_counter > 0:\n",
    "            empty_counter -= 1\n",
    "            if empty_counter == 0:\n",
    "                print(' ',end='')\n",
    "                transcription = transcription + ' '\n",
    "        return (in_data, pa.paContinue)\n",
    "\n",
    "    stream = p.open(format=pa.paInt16,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=SAMPLE_RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=dev_idx,\n",
    "                    stream_callback=callback,\n",
    "                    frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    print('Listening...')\n",
    "\n",
    "    stream.start_stream()\n",
    "    \n",
    "    # Interrupt kernel and then speak for a few more words to exit the pyaudio loop !\n",
    "    try:\n",
    "        while stream.is_active():\n",
    "            time.sleep(0.1)\n",
    "    finally:        \n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        print()\n",
    "        print(\"PyAudio stopped\")\n",
    "        print(transcription)\n",
    "    \n",
    "else:\n",
    "    print('ERROR: No audio input device found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NeMo_Online.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
